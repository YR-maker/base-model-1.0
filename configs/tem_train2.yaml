# 默认配置继承链
defaults:
  - train
  - override data: imageCAS
  - _self_

# 实验参数
num_shots: 3
lr: 0.001          # 【建议】微调 Adapter 时学习率可以稍微大一点，比如 1e-3 或 1e-4
devices: [0]
run_name: adapter_finetune

# 路径配置
# 注意：我们将路径设为 null，因为我们在 Wrapper 内部手动加载
path_to_chkpt: null
# 真实的预训练权重路径，传给 Wrapper
pretrained_weight_path: /home/yangrui/Project/Base-models/local_results/checkpoints/base.pt
chkpt_folder: /home/yangrui/Project/Base-models/local_results/checkpoints

dataloader:
  shuffle: False

# === 【核心修改】模型定义替换为 Wrapper ===
model:
  _target_: models.wrapper.AnatomyAwareVesselFM
  pretrained_path: ${pretrained_weight_path}

  # 这里我们需要把 dyn_unet_base.yaml 里的参数传进去
  # 必须保证这些参数和预训练时的结构完全一致！
  base_model_config:
    spatial_dims: 3
    in_channels: 1
    out_channels: 1
    strides:
      - [1, 1, 1]
      - [2, 2, 2]
      - [2, 2, 2]
      - [2, 2, 2]
      - [2, 2, 2]
      - [2, 2, 2]
    kernel_size:
      - [3, 3, 3]
      - [3, 3, 3]
      - [3, 3, 3]
      - [3, 3, 3]
      - [3, 3, 3]
      - [3, 3, 3]
    upsample_kernel_size:
      - [2, 2, 2]
      - [2, 2, 2]
      - [2, 2, 2]
      - [2, 2, 2]
      - [2, 2, 2]
    filters:
      - 32
      - 64
      - 128
      - 256
      - 320
      - 320
    res_block: True
# ========================================

trainer:
  lightning_trainer:
    val_check_interval: 50
    max_steps: 5000         # 稍微增加一点步数，让 Adapter 收敛
    max_epochs: 100         # 适当增加 Epoch

  lightning_module:
    _target_: utils.module.PLModuleFinetune
    _partial_: True
    prediction_threshold: 0.5
    batch_size: ${batch_size}
    input_size: ${input_size}

    # 优化器配置：Adapter 参数较少，可以使用稍大的 LR
    optimizer_factory:
      _target_: torch.optim.AdamW
      _partial_: True
      lr: ${lr}
      weight_decay: 0.01

    scheduler_configs:
      cosine_annealing_few:
        interval: step
        frequency: 10
        scheduler:
          _target_: torch.optim.lr_scheduler.CosineAnnealingLR
          _partial_: True
          T_max: 5000
          eta_min: 1e-6
          last_epoch: -1
      linear_warmup: null
      cosine_annealing: null