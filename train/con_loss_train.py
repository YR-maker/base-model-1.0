import logging
import sys
import warnings
import os

#è¯¥è®­ç»ƒæ˜¯åªä½¿ç”¨äº†è¿é€šæ€§é‚£ç¯‡è®ºæ–‡çš„æŸå¤±å‡½æ•°

from pathlib import Path

# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
current_file_path = Path(__file__).resolve()
# è·å–é¡¹ç›®æ ¹ç›®å½• (å³ fine-tuning æ–‡ä»¶å¤¹çš„ä¸Šä¸€çº§)
project_root = current_file_path.parent.parent
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° python æœç´¢è·¯å¾„ä¸­
sys.path.append(str(project_root))
# ==========================================
# ã€å…³é”®ä¿®å¤ã€‘MONAI ä¸ NumPy ç‰ˆæœ¬å…¼å®¹æ€§ä¿®å¤
# å¿…é¡»æ”¾åœ¨ from utils.dataset import UnionDataset ä¹‹å‰
try:
    import monai.transforms.transform

    # å¼ºåˆ¶ä¿®æ”¹ MONAI å†…éƒ¨çš„ MAX_SEEDï¼Œé˜²æ­¢ NumPy æŠ¥é”™ (OverflowError)
    monai.transforms.transform.MAX_SEED = 0xFFFFFFFF  # å³ 4294967295
except ImportError:
    pass
# ==========================================
import hydra
import torch
import torch.utils
from omegaconf import OmegaConf
from torch.utils.data import RandomSampler, Subset

from lightning.pytorch import seed_everything
from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint
from lightning.pytorch.loggers import WandbLogger, CSVLogger

from utils.dataset import UnionDataset
from utils.evaluation import Evaluator

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)


# åœ¨mainå‡½æ•°ä¹‹å‰å®šä¹‰è¾…åŠ©å‡½æ•°
def _log_validation_details(phase, trainer, pl_module, dataset_name):
    """è®°å½•éªŒè¯è¯¦ç»†ç»“æœ"""
    # ä»…åœ¨ä¸»è¿›ç¨‹æ‰“å°
    if trainer.global_rank != 0:
        return

    current_metrics = trainer.callback_metrics
    dice_score = current_metrics.get(f"{dataset_name}_val_dice", None)
    val_dice_metric = current_metrics.get("val_DiceMetric", None)

    logger.info("ğŸ“ˆ " + "=" * 50)
    logger.info(f"ğŸ“‹ {phase}ç»“æœæ‘˜è¦")
    logger.info("ğŸ“ˆ " + "=" * 50)
    logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
    if dice_score is not None:
        logger.info(f"ğŸ¯ æ•°æ®é›†Dice: {dice_score:.4f}")
    if val_dice_metric is not None:
        logger.info(f"â­ ç»¼åˆDiceæŒ‡æ ‡: {val_dice_metric:.4f}")
    logger.info("ğŸ“ˆ " + "=" * 50)


def _log_test_summary(trainer, pl_module, dataset_name):
    """è®°å½•æµ‹è¯•ç»“æœæ‘˜è¦"""
    # ä»…åœ¨ä¸»è¿›ç¨‹æ‰“å°
    if trainer.global_rank != 0:
        return

    current_metrics = trainer.callback_metrics
    test_dice = current_metrics.get(f"{dataset_name}_test_dice", None)
    test_dice_metric = current_metrics.get("test_DiceMetric", None)

    logger.info("ğŸ‰ " + "=" * 60)
    logger.info("ğŸ† æœ€ç»ˆæµ‹è¯•ç»“æœæŠ¥å‘Š")
    logger.info("ğŸ‰ " + "=" * 60)
    if test_dice is not None:
        logger.info(f"âœ… æµ‹è¯•é›†Diceåˆ†æ•°: {test_dice:.4f}")
    if test_dice_metric is not None:
        logger.info(f"ğŸ… æœ€ç»ˆDiceæŒ‡æ ‡: {test_dice_metric:.4f}")
    logger.info("ğŸ‰ " + "=" * 60)


@hydra.main(config_path="../configs", config_name="con_loss_train", version_base="1.3.2")
def main(cfg):
    """
    æ¨¡å‹çš„å¾®è°ƒä¸»å‡½æ•° (å·²é€‚é…å¤šå¡DDPè®­ç»ƒ)
    """

    # è®¾ç½®éšæœºç§å­ç¡®ä¿å®éªŒå¯å¤ç°æ€§
    seed_everything(cfg.seed, True)
    # è®¾ç½®çŸ©é˜µä¹˜æ³•ç²¾åº¦å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
    torch.set_float32_matmul_precision("medium")

    # è·å–å½“å‰è¿›ç¨‹çš„å…¨å±€ rankï¼Œç”¨äºæ§åˆ¶æ—¥å¿—æ‰“å°
    # Lightning åˆå§‹åŒ–å‰å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è·å–ï¼Œé»˜è®¤ä¸º 0
    global_rank = int(os.environ.get("RANK", 0))

    # æ„å»ºè¿è¡Œåç§°ï¼ŒåŒ…å«å…³é”®å®éªŒä¿¡æ¯
    dataset_name = list(cfg.data.keys())[0]  # è·å–æ•°æ®é›†åç§°
    # === ã€æ–°å¢ä»£ç ã€‘è·å–è·¯å¾„æœ€åä¸€éƒ¨åˆ† ===
    # 1. è·å–å®Œæ•´è·¯å¾„å­—ç¬¦ä¸² (ä¾‹å¦‚: /home/yangrui/Project/Base-model/input/imageCAS)
    full_data_path = cfg.data[dataset_name].path

    # 2. ä½¿ç”¨ Path å¯¹è±¡æå–æœ€åä¸€ä¸ªæ–‡ä»¶å¤¹å (ä¾‹å¦‚: imageCAS)
    # Path(è·¯å¾„).name ä¼šè‡ªåŠ¨è·å–è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†
    last_folder_name = os.path.basename(os.path.normpath(full_data_path))

    run_name = f'{cfg.loss_name}_{cfg.num_shots}shot_{last_folder_name}'

    # å¼ºåˆ¶è®¾ç½®ä¸ºç¦»çº¿æ¨¡å¼
    cfg.offline = True

    # ---------------------------------------------------------
    # ã€ä¿®æ”¹ç‚¹ã€‘è®¾ç½®æ—¥å¿—ä¿å­˜çš„ç»å¯¹è·¯å¾„ (ä»… Rank 0 åˆ›å»ºç›®å½•)
    # ---------------------------------------------------------
    save_root_dir = "/home/yangrui/Project/Base-model/local_results/doc"
    if global_rank == 0:
        os.makedirs(save_root_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        logger.info(f"ğŸ“‚ æ—¥å¿—å­˜å‚¨è·¯å¾„å·²è®¾ç½®ä¸º: {save_root_dir}")

    # åˆå§‹åŒ–Weights & Biasesæ—¥å¿—è®°å½•å™¨ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
    # Lightning ä¼šè‡ªåŠ¨å¤„ç† Logger çš„å¤šè¿›ç¨‹é€»è¾‘ï¼Œæ— éœ€æ‰‹åŠ¨é™åˆ¶ rank
    wnb_logger = WandbLogger(
        save_dir=save_root_dir,
        project=cfg.wandb_project,
        name=run_name,
        config=OmegaConf.to_container(cfg),
        offline=True,
        mode="offline"
    )

    # åŒæ—¶æ·»åŠ CSVæ—¥å¿—è®°å½•å™¨
    csv_logger = CSVLogger(
        save_dir=save_root_dir,
        name=run_name,
        version="version_0"
    )

    # è®¾ç½®è®­ç»ƒå›è°ƒå‡½æ•°
    lr_monitor = LearningRateMonitor()
    monitor_metric = "val_DiceMetric"

    # è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼Œç”¨äºæ‰“å°éªŒè¯ç»“æœ
    class ValidationResultCallback(LearningRateMonitor):
        def on_validation_end(self, trainer, pl_module):
            # ä»…åœ¨ä¸»è¿›ç¨‹æ‰“å°æ—¥å¿—
            if trainer.global_rank != 0:
                return

            # è·å–å½“å‰éªŒè¯æŒ‡æ ‡
            current_metrics = trainer.callback_metrics
            dice_score = current_metrics.get(f"{dataset_name}_val_dice", None)
            val_dice_metric = current_metrics.get("val_DiceMetric", None)
            val_loss = current_metrics.get(f"{dataset_name}_val_loss", None)

            # è·å–å½“å‰è®­ç»ƒæ­¥æ•°å’Œepoch
            current_step = trainer.global_step
            current_epoch = trainer.current_epoch

            # æ‰“å°è¯¦ç»†çš„éªŒè¯ç»“æœ
            logger.info("=" * 60)
            logger.info("ğŸ“Š éªŒè¯ç»“æœæŠ¥å‘Š")
            logger.info("=" * 60)
            logger.info(f"ğŸƒâ€â™‚ï¸ å½“å‰è®­ç»ƒè¿›åº¦: Epoch {current_epoch} | Step {current_step}")
            logger.info(f"ğŸ¯ æ•°æ®é›†: {dataset_name}")

            if dice_score is not None:
                logger.info(f"âœ… {dataset_name} Diceåˆ†æ•°: {dice_score:.4f}")
            if val_dice_metric is not None:
                logger.info(f"ğŸ† éªŒè¯DiceæŒ‡æ ‡: {val_dice_metric:.4f}")
            if val_loss is not None:
                logger.info(f"ğŸ“‰ éªŒè¯æŸå¤±å€¼: {val_loss:.4f}")

            # æ‰“å°æœ€ä½³æŒ‡æ ‡å¯¹æ¯”
            if hasattr(trainer, 'checkpoint_callback') and trainer.checkpoint_callback is not None:
                best_dice = trainer.checkpoint_callback.best_model_score
                if best_dice is not None:
                    logger.info(f"â­ å†å²æœ€ä½³Dice: {best_dice:.4f}")
                    if val_dice_metric is not None:
                        improvement = val_dice_metric - best_dice
                        if improvement > 0:
                            logger.info(f"ğŸš€ ç›¸æ¯”æœ€ä½³æå‡: +{improvement:.4f}")
                        else:
                            logger.info(f"ğŸ“Œ è·ç¦»æœ€ä½³ç›¸å·®: {improvement:.4f}")

            logger.info("=" * 60)

    # æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ - ä¿å­˜æœ€ä½³æ¨¡å‹
    checkpoint_callback = ModelCheckpoint(
        dirpath=cfg.chkpt_folder + "/" + cfg.data_name + "/" + last_folder_name + "/" + run_name,
        monitor=monitor_metric,
        save_top_k=1,
        mode="max",
        filename="{step}_{" + monitor_metric + ":.2f}",
        auto_insert_metric_name=True,
        save_last=True
    )
    checkpoint_callback.CHECKPOINT_EQUALS_CHAR = ":"
    checkpoint_callback.CHECKPOINT_NAME_LAST = run_name + "_last"

    # åˆå§‹åŒ–éªŒè¯ç»“æœå›è°ƒ
    validation_callback = ValidationResultCallback()

    # ---------------------------------------------------------
    # ã€ä¿®æ”¹ç‚¹ã€‘é…ç½®å¤šå¡è®­ç»ƒå‚æ•°
    # ---------------------------------------------------------
    # ç¡®å®šä½¿ç”¨çš„ GPU æ•°é‡
    num_devices = cfg.devices_num  # å¼ºåˆ¶è®¾ç½®ä¸º 2 å¡ï¼Œæˆ–è€…è¯»å– len(cfg.devices)

    # å®ä¾‹åŒ– Trainer
    trainer_cls = hydra.utils.instantiate(cfg.trainer.lightning_trainer)

    # è¦†ç›–å‚æ•°ä»¥å¯ç”¨ DDP å¤šå¡è®­ç»ƒ
    trainer_additional_kwargs = {
        "logger": [wnb_logger, csv_logger],
        "callbacks": [lr_monitor, checkpoint_callback, validation_callback],
        "devices": num_devices,  # ä½¿ç”¨4å¼ æ˜¾å¡
        "accelerator": "gpu",  # åŠ é€Ÿå™¨ç±»å‹
        "strategy": "ddp",  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œç­–ç•¥
        "sync_batchnorm": True,  # ã€é‡è¦ã€‘å¤šå¡åŒæ­¥BatchNormï¼Œå¯¹åˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦
        "use_distributed_sampler": False  # ã€é‡è¦ã€‘ç¦ç”¨é»˜è®¤é‡‡æ ·å™¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰RandomSampler
    }
    # å¦‚æœ cfg ä¸­å·²ç»å®ä¾‹åŒ–äº† trainer å¯¹è±¡ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦è°ƒæ•´å†™æ³•
    # é€šå¸¸ hydra instantiate è¿”å›çš„æ˜¯å¯¹è±¡ï¼Œè¿™é‡Œå‡è®¾å®ƒè¿”å›çš„æ˜¯ partial æˆ–è€…æˆ‘ä»¬é‡æ–°æ„é€ 
    # ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ Trainer ç±»å°è£…å‚æ•°ï¼Œæˆ–è€…æ²¿ç”¨åŸé€»è¾‘è¦†ç›–
    # åŸé€»è¾‘æ˜¯: trainer = hydra... -> trainer(**kwargs)
    # è¿™é‡Œçš„ cfg.trainer.lightning_trainer åº”è¯¥æ˜¯ä¸€ä¸ª _partial_: True çš„é…ç½®
    trainer = trainer_cls(**trainer_additional_kwargs)

    # ---------------------------------------------------------
    # ã€ä¿®æ”¹ç‚¹ã€‘è°ƒæ•´æ•°æ®é‡‡æ ·å™¨ä»¥é€‚é…å¤šå¡
    # ---------------------------------------------------------
    train_dataset = UnionDataset(cfg.data, "train", finetune=True)
    train_dataset = Subset(train_dataset, range(cfg.num_shots))

    # è®¡ç®—æ¯å¼ å¡éœ€è¦è·‘çš„æ ·æœ¬æ•°ï¼Œä¿æŒæ€» Epoch è§„æ¨¡ä¸å˜ (çº¦10000)
    total_samples_per_epoch = int(1e5)
    samples_per_gpu = total_samples_per_epoch // num_devices

    if global_rank == 0:
        logger.info(f"Train dataset size mapped to {len(train_dataset)} samples")
        logger.info(f"Multi-GPU Config: {num_devices} GPUs")
        logger.info(
            f"Sampler: {samples_per_gpu} samples per GPU (Total effective epoch size: {samples_per_gpu * num_devices})")

    # ä½¿ç”¨éšæœºé‡‡æ ·å™¨å¹¶è¿›è¡Œé‡å¤é‡‡æ ·
    # æ³¨æ„ï¼šåœ¨DDPæ¨¡å¼ä¸‹ï¼Œå¦‚æœä¸ä½¿ç”¨DistributedSamplerï¼Œæ¯å¼ å¡éƒ½ä¼šç‹¬ç«‹è¿›è¡ŒRandomSampling
    # å› ä¸ºæˆ‘ä»¬æ˜¯ replacement=True ä¸”æ ·æœ¬æå°‘ï¼Œè¿™ç§ç‹¬ç«‹éšæœºæ˜¯å®Œå…¨å¯ä»¥æ¥å—çš„
    random_sampler = RandomSampler(train_dataset, replacement=True, num_samples=samples_per_gpu)

    train_loader = hydra.utils.instantiate(cfg.dataloader)(
        dataset=train_dataset,
        sampler=random_sampler,
        # å»ºè®®åœ¨å¤šå¡è®­ç»ƒæ—¶é€‚å½“å¢åŠ  num_workers
        # num_workers=4
    )

    # éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†
    val_dataset = UnionDataset(cfg.data, "val", finetune=True)
    val_loader = hydra.utils.instantiate(cfg.dataloader)(dataset=val_dataset, batch_size=1)
    if global_rank == 0: logger.info(f"Val dataset size: {len(val_dataset)}")

    test_dataset = UnionDataset(cfg.data, "test", finetune=True)
    test_loader = hydra.utils.instantiate(cfg.dataloader)(dataset=test_dataset, batch_size=1)
    if global_rank == 0: logger.info(f"Test dataset size: {len(test_dataset)}")

    # åˆå§‹åŒ–æ¨¡å‹
    model = hydra.utils.instantiate(cfg.model)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæŒ‡å®šäº†æ£€æŸ¥ç‚¹è·¯å¾„ï¼‰
    if cfg.path_to_chkpt is not None:
        # ã€ä¿®æ”¹ã€‘åŠ è½½æƒé‡æ—¶æ˜ å°„åˆ° CPUï¼Œé¿å…å¤šè¿›ç¨‹å ç”¨å¯¼è‡´çš„é—®é¢˜ï¼Œéšå Lightning ä¼šè‡ªåŠ¨è½¬åˆ° GPU
        try:
            chkpt = torch.load(cfg.path_to_chkpt, map_location='cpu', weights_only=True)
        except:
            chkpt = torch.load(cfg.path_to_chkpt, map_location='cpu', weights_only=False)

        # å¤„ç†çŠ¶æ€å­—å…¸
        if isinstance(chkpt, dict):
            model_chkpt = chkpt.get('state_dict', chkpt.get('model_state_dict', chkpt.get('models', chkpt)))
        else:
            model_chkpt = chkpt

        # ç§»é™¤"models."å‰ç¼€ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if isinstance(model_chkpt, dict) and any(k.startswith('models.') for k in model_chkpt.keys()):
            from collections import OrderedDict
            model_chkpt = OrderedDict([(k.replace('models.', '', 1) if k.startswith('models.') else k, v)
                                       for k, v in model_chkpt.items()])

        model.load_state_dict(model_chkpt, strict=False)
        if global_rank == 0:
            logger.info(f"Loaded pretrained weights from {cfg.path_to_chkpt}")

    # åˆå§‹åŒ–Lightningæ¨¡å—
    evaluator = Evaluator()
    lightning_module = hydra.utils.instantiate(cfg.trainer.lightning_module)(
        model=model,
        evaluator=evaluator,
        dataset_name=dataset_name
    )

    # è®­ç»ƒæµç¨‹
    if not cfg.offline:
        if global_rank == 0:
            wnb_logger.watch(model, log="all", log_freq=20)
    else:
        if global_rank == 0:
            logger.info("ç¦»çº¿æ¨¡å¼ï¼šè·³è¿‡æ¨¡å‹å‚æ•°ç›‘æ§")

        # æ ¹æ®æ ·æœ¬æ•°é‡é€‰æ‹©ä¸åŒçš„å®éªŒæ¨¡å¼
        if cfg.num_shots == 0:
            if global_rank == 0: logger.info("Starting zero-shot evaluation")
            trainer.test(lightning_module, test_loader)
        else:
            if global_rank == 0:
                logger.info("Starting training")
                logger.info("ğŸ” è¿›è¡Œåˆå§‹éªŒè¯...")

            # åˆå§‹éªŒè¯
            trainer.validate(lightning_module, val_loader)

            if global_rank == 0:
                _log_validation_details("åˆå§‹éªŒè¯", trainer, lightning_module, dataset_name)
                logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")

            # å¼€å§‹è®­ç»ƒ
            trainer.fit(lightning_module, train_loader, val_loader)

            if global_rank == 0:
                logger.info("Finished training")
                logger.info("ğŸ§ª è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")

            # æœ€ç»ˆæµ‹è¯•
            trainer.test(lightning_module, test_loader, ckpt_path="best")

            if global_rank == 0:
                _log_test_summary(trainer, lightning_module, dataset_name)
                logger.info(f"å®éªŒå®Œæˆï¼æ—¥å¿—ä¿å­˜åœ¨ï¼š{save_root_dir}")


if __name__ == "__main__":
    # è®¾ç½®æ ‡å‡†è¾“å‡ºç¼“å†²ï¼Œç¡®ä¿æ—¥å¿—å®æ—¶æ˜¾ç¤º
    sys.stdout = open(sys.stdout.fileno(), mode="w", buffering=1)
    sys.stderr = open(sys.stderr.fileno(), mode="w", buffering=1)

    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢wandbè¯¢é—®
    import os

    os.environ["WANDB_SILENT"] = "true"
    os.environ["WANDB_MODE"] = "offline"

    main()