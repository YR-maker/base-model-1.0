import logging
import sys
import warnings
import os
import numpy as np
from pathlib import Path

# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
current_file_path = Path(__file__).resolve()
# è·å–é¡¹ç›®æ ¹ç›®å½• (å³ train æ–‡ä»¶å¤¹çš„ä¸Šä¸€çº§)
project_root = current_file_path.parent.parent
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° python æœç´¢è·¯å¾„ä¸­
sys.path.append(str(project_root))

# ==========================================
# ã€å…³é”®ä¿®å¤ã€‘MONAI ä¸ NumPy ç‰ˆæœ¬å…¼å®¹æ€§ä¿®å¤
# å¿…é¡»æ”¾åœ¨ from utils.dataset import UnionDataset ä¹‹å‰
try:
    import monai.transforms.transform

    # å¼ºåˆ¶ä¿®æ”¹ MONAI å†…éƒ¨çš„ MAX_SEEDï¼Œé˜²æ­¢ NumPy æŠ¥é”™ (OverflowError)
    monai.transforms.transform.MAX_SEED = 0xFFFFFFFF
except ImportError:
    pass
# ==========================================

import hydra
import torch
import torch.utils
from omegaconf import OmegaConf
from torch.utils.data import RandomSampler

from lightning.pytorch import seed_everything
from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint
from lightning.pytorch.loggers import WandbLogger, CSVLogger

from utils.dataset import UnionDataset
from utils.evaluation import Evaluator

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)


def _log_validation_details(phase, trainer, pl_module, dataset_name):
    """è®°å½•éªŒè¯è¯¦ç»†ç»“æœ"""
    if trainer.global_rank != 0:
        return

    current_metrics = trainer.callback_metrics
    dice_score = current_metrics.get(f"{dataset_name}_val_dice", None)
    val_dice_metric = current_metrics.get("val_DiceMetric", None)

    logger.info("ğŸ“ˆ " + "=" * 50)
    logger.info(f"ğŸ“‹ {phase}ç»“æœæ‘˜è¦")
    logger.info("ğŸ“ˆ " + "=" * 50)
    logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
    if dice_score is not None:
        logger.info(f"ğŸ¯ æ•°æ®é›†Dice: {dice_score:.4f}")
    if val_dice_metric is not None:
        logger.info(f"â­ ç»¼åˆDiceæŒ‡æ ‡: {val_dice_metric:.4f}")
    logger.info("ğŸ“ˆ " + "=" * 50)


def _log_test_summary(trainer, pl_module, dataset_name):
    """è®°å½•æµ‹è¯•ç»“æœæ‘˜è¦"""
    if trainer.global_rank != 0:
        return

    current_metrics = trainer.callback_metrics
    test_dice = current_metrics.get(f"{dataset_name}_test_dice", None)
    test_dice_metric = current_metrics.get("test_DiceMetric", None)

    logger.info("ğŸ‰ " + "=" * 60)
    logger.info("ğŸ† æœ€ç»ˆæµ‹è¯•ç»“æœæŠ¥å‘Š")
    logger.info("ğŸ‰ " + "=" * 60)
    if test_dice is not None:
        logger.info(f"âœ… æµ‹è¯•é›†Diceåˆ†æ•°: {test_dice:.4f}")
    if test_dice_metric is not None:
        logger.info(f"ğŸ… æœ€ç»ˆDiceæŒ‡æ ‡: {test_dice_metric:.4f}")
    logger.info("ğŸ‰ " + "=" * 60)


@hydra.main(config_path="../configs", config_name="mutil_train", version_base="1.3.2")
def main(cfg):
    """
    æ¨¡å‹çš„å¾®è°ƒä¸»å‡½æ•° (å·²é€‚é…å¤šå¡DDPè®­ç»ƒåŠåŠ¨æ€æ­¥æ•°è°ƒæ•´)
    """

    # è®¾ç½®éšæœºç§å­ç¡®ä¿å®éªŒå¯å¤ç°æ€§
    seed_everything(cfg.seed, True)
    # è®¾ç½®çŸ©é˜µä¹˜æ³•ç²¾åº¦å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
    torch.set_float32_matmul_precision("medium")

    # è·å–å½“å‰è¿›ç¨‹çš„å…¨å±€ rankï¼Œç”¨äºæ§åˆ¶æ—¥å¿—æ‰“å°
    global_rank = int(os.environ.get("RANK", 0))

    # æ„å»ºè¿è¡Œåç§°
    dataset_name = list(cfg.data.keys())[0]
    full_data_path = cfg.data[dataset_name].path
    last_folder_name = os.path.basename(os.path.normpath(full_data_path))
    run_name = f'{cfg.loss_name}_{cfg.num_shots}shot_{last_folder_name}'

    # å¼ºåˆ¶è®¾ç½®ä¸ºç¦»çº¿æ¨¡å¼
    cfg.offline = True

    # è®¾ç½®æ—¥å¿—ä¿å­˜è·¯å¾„
    save_root_dir = "/home/yangrui/Project/Base-model/local_results/doc/" + cfg.data_name
    if global_rank == 0:
        os.makedirs(save_root_dir, exist_ok=True)
        logger.info(f"ğŸ“‚ æ—¥å¿—å­˜å‚¨è·¯å¾„å·²è®¾ç½®ä¸º: {save_root_dir}")

    # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
    wnb_logger = WandbLogger(
        save_dir=save_root_dir,
        project=cfg.wandb_project,
        name=run_name,
        config=OmegaConf.to_container(cfg),
        offline=True,
        mode="offline"
    )

    csv_logger = CSVLogger(
        save_dir=save_root_dir,
        name=run_name,
        version="version_0"
    )

    # ---------------------------------------------------------
    # åŠ¨æ€è®¡ç®—æ­¥æ•°ç­–ç•¥
    # ---------------------------------------------------------
    target_devices = cfg.devices
    num_devices = len(target_devices)
    base_total_steps = cfg.trainer.lightning_trainer.max_steps

    # åŠ¨æ€è°ƒæ•´ max_steps å’Œ val_check_interval
    actual_max_steps = int(base_total_steps // num_devices)
    actual_val_interval = int(actual_max_steps // 25)

    if global_rank == 0:
        logger.info("=" * 40)
        logger.info(f"ğŸ§® åŠ¨æ€è®­ç»ƒç­–ç•¥è°ƒæ•´ (GPUæ•°é‡: {num_devices})")
        logger.info(f"   - YAMLåŸºå‡†æ­¥æ•°: {base_total_steps}")
        logger.info(f"   - å®é™…è®­ç»ƒæ­¥æ•°: {actual_max_steps}")
        logger.info(f"   - è¯„ä¼°é—´éš”: {actual_val_interval} steps")
        logger.info("=" * 40)

    # è®¾ç½®å›è°ƒå‡½æ•°
    lr_monitor = LearningRateMonitor()
    monitor_metric = "val_DiceMetric"

    class ValidationResultCallback(LearningRateMonitor):
        def on_validation_end(self, trainer, pl_module):
            if trainer.global_rank != 0: return

            # è·å–æŒ‡æ ‡å¹¶æ‰“å°
            current_metrics = trainer.callback_metrics
            # æ³¨æ„ï¼šè¿™é‡Œä» metrics å–å‡ºçš„å¯èƒ½æ˜¯ Tensorï¼Œè®¡ç®—æ—¶æœ€å¥½è½¬ä¸º float
            dice_score = current_metrics.get(f"{dataset_name}_val_dice", None)
            val_dice_metric = current_metrics.get("val_DiceMetric", None)
            val_loss = current_metrics.get(f"{dataset_name}_val_loss", None)
            current_epoch = trainer.current_epoch

            logger.info("=" * 60)
            logger.info(f"ğŸ“Š éªŒè¯ç»“æœæŠ¥å‘Š (Epoch {current_epoch})")
            if dice_score is not None: logger.info(f"âœ… {dataset_name} Dice: {dice_score:.4f}")
            if val_dice_metric is not None: logger.info(f"ğŸ† éªŒè¯DiceæŒ‡æ ‡: {val_dice_metric:.4f}")
            if val_loss is not None: logger.info(f"ğŸ“‰ éªŒè¯æŸå¤±å€¼: {val_loss:.4f}")

            # --- ä¿®æ”¹å¼€å§‹ï¼šæ·»åŠ å†å²æœ€ä½³ä¸å·®è·è®¡ç®— ---
            if hasattr(trainer, 'checkpoint_callback') and trainer.checkpoint_callback is not None:
                best_dice = trainer.checkpoint_callback.best_model_score

                if best_dice is not None:
                    logger.info(f"â­ å†å²æœ€ä½³Dice: {best_dice:.4f}")

                    # åªæœ‰å½“å½“å‰åˆ†æ•°ä¹Ÿå­˜åœ¨æ—¶ï¼Œæ‰è®¡ç®—å·®è·
                    if dice_score is not None:
                        current_val = float(dice_score)
                        best_val = float(best_dice)
                        diff = current_val - best_val

                        # æ ¼å¼åŒ–è¾“å‡ºï¼šå¦‚æœæ˜¯æ­£æ•°åŠ  '+' å·ï¼Œä¸”ç”¨ä¸åŒå›¾æ ‡è¡¨ç¤º
                        if diff > 0:
                            logger.info(f"ğŸš€ æ–°çºªå½•! æå‡: +{diff:.4f}")
                        elif diff == 0:
                            logger.info(f"âš–ï¸  æŒå¹³å†å²æœ€ä½³")
                        else:
                            logger.info(f"ğŸ”™ è·å†å²æœ€ä½³: {diff:.4f}")
            # --- ä¿®æ”¹ç»“æŸ ---

            logger.info("=" * 60)

    checkpoint_callback = ModelCheckpoint(
        dirpath=cfg.chkpt_folder + "/" + cfg.data_name + "/" + last_folder_name + "/" + run_name,
        monitor=monitor_metric,
        save_top_k=1,
        mode="max",
        filename="{step}_{" + monitor_metric + ":.2f}_" + f"{num_devices}GPUs",
        auto_insert_metric_name=True,
        save_last=True
    )
    checkpoint_callback.CHECKPOINT_EQUALS_CHAR = ":"
    checkpoint_callback.CHECKPOINT_NAME_LAST = run_name + "_last"

    validation_callback = ValidationResultCallback()

    # å®ä¾‹åŒ– Trainer
    trainer_cls = hydra.utils.instantiate(cfg.trainer.lightning_trainer)
    trainer_additional_kwargs = {
        "logger": [wnb_logger, csv_logger],
        "callbacks": [lr_monitor, checkpoint_callback, validation_callback],
        "max_steps": actual_max_steps,
        "val_check_interval": actual_val_interval,
        "devices": target_devices,
        "accelerator": "gpu",
        "strategy": "ddp",
        "sync_batchnorm": True,
        "use_distributed_sampler": False
    }
    trainer = trainer_cls(**trainer_additional_kwargs)

    # ---------------------------------------------------------
    # æ•°æ®é›†å®šä¹‰ (å†…å­˜åŠ è½½ç±»)
    # ---------------------------------------------------------
    class FewShotInMemoryDataset(torch.utils.data.Dataset):
        def __init__(self, data_list, transform):
            self.data = data_list
            self.transform = transform

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            item = self.data[idx]
            # å®æ—¶åº”ç”¨å˜æ¢
            transformed = self.transform(item)
            # å¿…é¡»è¿”å› (Image, Mask) å…ƒç»„
            return transformed['Image'], transformed['Mask'] > 0

    # ---------------------------------------------------------
    # 1. è®­ç»ƒé›† (Few-Shot): åŠ è½½åˆ°å†…å­˜
    # ---------------------------------------------------------
    raw_train_dataset = UnionDataset(cfg.data, "train", finetune=True)
    dataset_info = raw_train_dataset.datasets[0]
    data_paths = dataset_info["paths"]
    reader = dataset_info["reader"]
    data_transform = dataset_info["transforms"]

    subset_data_list = []
    shots_to_load = min(cfg.num_shots, len(data_paths))

    if global_rank == 0:
        logger.info(f"ğŸš€ æ­£åœ¨åŠ è½½ {shots_to_load} ä¸ªè®­ç»ƒæ ·æœ¬åˆ°å†…å­˜...")

    for i in range(shots_to_load):
        sample_path = data_paths[i]
        img_path = [p for p in sample_path.iterdir() if 'img' in p.name][0]
        mask_path = [p for p in sample_path.iterdir() if 'label' in p.name][0]

        img = reader.read_images(str(img_path))[0].astype(np.float32)
        mask = reader.read_images(str(mask_path))[0].astype(bool)
        subset_data_list.append({'Image': img, 'Mask': mask})

    train_dataset = FewShotInMemoryDataset(data_list=subset_data_list, transform=data_transform)

    # é‡‡æ ·å™¨é…ç½®
    total_samples_per_epoch = int(1e5)
    samples_per_gpu = total_samples_per_epoch // num_devices
    random_sampler = RandomSampler(train_dataset, replacement=True, num_samples=samples_per_gpu)

    train_loader = hydra.utils.instantiate(cfg.dataloader)(
        dataset=train_dataset,
        sampler=random_sampler
    )

    # ---------------------------------------------------------
    # 2. éªŒè¯é›† (Validation): åŠ è½½åˆ°å†…å­˜ (åŠ é€Ÿè¯„ä¼°)
    # ---------------------------------------------------------
    def _load_split_to_memory(cfg, phase, global_rank):
        """é€šç”¨è¾…åŠ©å‡½æ•°ï¼šå°†æ•°æ®é›†åŠ è½½åˆ°å†…å­˜"""
        raw_dataset = UnionDataset(cfg.data, phase, finetune=True)
        if not raw_dataset.datasets or len(raw_dataset) == 0:
            return None

        d_info = raw_dataset.datasets[0]
        d_paths = d_info["paths"]
        d_reader = d_info["reader"]
        d_transform = d_info["transforms"]
        d_list = []

        if global_rank == 0:
            logger.info(f"ğŸš€ æ­£åœ¨å°† {phase} é›† ({len(d_paths)} æ ·æœ¬) åŠ è½½åˆ°å†…å­˜...")

        for s_path in d_paths:
            i_path = [p for p in s_path.iterdir() if 'img' in p.name][0]
            m_path = [p for p in s_path.iterdir() if 'label' in p.name][0]
            img = d_reader.read_images(str(i_path))[0].astype(np.float32)
            mask = d_reader.read_images(str(m_path))[0].astype(bool)
            d_list.append({'Image': img, 'Mask': mask})

        return FewShotInMemoryDataset(data_list=d_list, transform=d_transform)

    val_dataset = _load_split_to_memory(cfg, "val", global_rank)
    if val_dataset is not None:
        val_loader = hydra.utils.instantiate(cfg.dataloader)(dataset=val_dataset, batch_size=1)
        if global_rank == 0: logger.info(f"Val dataset size (In-Memory): {len(val_dataset)}")
    else:
        val_loader = None

    # ---------------------------------------------------------
    # 3. æµ‹è¯•é›† (Test): ä¿æŒç¡¬ç›˜è¯»å– (èŠ‚çœå†…å­˜)
    # ---------------------------------------------------------
    test_dataset = UnionDataset(cfg.data, "test", finetune=True)
    test_loader = hydra.utils.instantiate(cfg.dataloader)(dataset=test_dataset, batch_size=1)
    if global_rank == 0: logger.info(f"Test dataset size (Disk-Based): {len(test_dataset)}")

    # åˆå§‹åŒ–æ¨¡å‹
    model = hydra.utils.instantiate(cfg.model)

    # ---------------------------------------------------------
    # æƒé‡åŠ è½½é€»è¾‘
    # ---------------------------------------------------------
    if cfg.path_to_chkpt is not None:
        if global_rank == 0: logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½æƒé‡: {cfg.path_to_chkpt}")
        try:
            chkpt = torch.load(cfg.path_to_chkpt, map_location='cpu', weights_only=True)
        except (TypeError, Exception):
            chkpt = torch.load(cfg.path_to_chkpt, map_location='cpu', weights_only=False)

        if isinstance(chkpt, dict) and 'state_dict' in chkpt:
            model_chkpt = chkpt['state_dict']
            is_lightning = True
        else:
            model_chkpt = chkpt.get('state_dict', chkpt.get('model_state_dict', chkpt.get('models', chkpt)))
            is_lightning = False

        new_state_dict = {}
        for k, v in model_chkpt.items():
            new_key = k
            if is_lightning and k.startswith('model.'):
                new_key = k.replace('model.', '', 1)
            elif k.startswith('models.'):
                new_key = k.replace('models.', '', 1)
            new_state_dict[new_key] = v

        try:
            model.load_state_dict(new_state_dict, strict=False)
            if global_rank == 0: logger.info(f"âœ… æˆåŠŸåŠ è½½æƒé‡")
        except RuntimeError as e:
            # æ™ºèƒ½å‰”é™¤ä¸åŒ¹é…çš„å±‚
            if global_rank == 0: logger.warning(f"âš ï¸ å®Œæ•´åŠ è½½å¤±è´¥ï¼Œå°è¯•æ™ºèƒ½å‰”é™¤...")
            current_model_dict = model.state_dict()
            filtered_dict = {k: v for k, v in new_state_dict.items()
                             if k in current_model_dict and v.shape == current_model_dict[k].shape}
            model.load_state_dict(filtered_dict, strict=False)
            if global_rank == 0: logger.info("âœ… å·²åŠ è½½åŒ¹é…å±‚")

    # åˆå§‹åŒ– LightningModule
    evaluator = Evaluator()
    lightning_module = hydra.utils.instantiate(cfg.trainer.lightning_module)(
        model=model,
        evaluator=evaluator,
        dataset_name=dataset_name
    )

    # è®­ç»ƒæµç¨‹
    if not cfg.offline:
        if global_rank == 0: wnb_logger.watch(model, log="all", log_freq=20)
    else:
        if global_rank == 0: logger.info("ç¦»çº¿æ¨¡å¼ï¼šè·³è¿‡æ¨¡å‹å‚æ•°ç›‘æ§")

    if cfg.num_shots == 0:
        if global_rank == 0: logger.info("Starting zero-shot evaluation")
        trainer.test(lightning_module, test_loader)
    else:
        if global_rank == 0:
            logger.info("Starting training")
            logger.info("ğŸ” è¿›è¡Œåˆå§‹éªŒè¯...")

        trainer.validate(lightning_module, val_loader)
        if global_rank == 0:
            _log_validation_details("åˆå§‹éªŒè¯", trainer, lightning_module, dataset_name)
            logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")

        trainer.fit(lightning_module, train_loader, val_loader)

        if global_rank == 0:
            logger.info("Finished training")
            logger.info("ğŸ§ª è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")

        trainer.test(lightning_module, test_loader, ckpt_path="best")
        if global_rank == 0:
            _log_test_summary(trainer, lightning_module, dataset_name)
            logger.info(f"å®éªŒå®Œæˆï¼æ—¥å¿—ä¿å­˜åœ¨ï¼š{save_root_dir}")


if __name__ == "__main__":
    sys.stdout = open(sys.stdout.fileno(), mode="w", buffering=1)
    sys.stderr = open(sys.stderr.fileno(), mode="w", buffering=1)
    os.environ["WANDB_SILENT"] = "true"
    os.environ["WANDB_MODE"] = "offline"
    main()