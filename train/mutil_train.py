import logging
import sys
import warnings
import os  # ç¡®ä¿å¯¼å…¥ os
import numpy as np
from pathlib import Path
from monai.data import CacheDataset # ä¿ç•™å¼•ç”¨ï¼Œè™½ç„¶ä¸‹é¢è¢«è‡ªå®šä¹‰ç±»æ›¿ä»£ï¼Œä½†ä¿æŒå…¼å®¹æ€§

# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
current_file_path = Path(__file__).resolve()
# è·å–é¡¹ç›®æ ¹ç›®å½• (å³ train æ–‡ä»¶å¤¹çš„ä¸Šä¸€çº§)
project_root = current_file_path.parent.parent
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° python æœç´¢è·¯å¾„ä¸­
sys.path.append(str(project_root))


# ==========================================
# ã€å…³é”®ä¿®å¤ã€‘MONAI ä¸ NumPy ç‰ˆæœ¬å…¼å®¹æ€§ä¿®å¤
# å¿…é¡»æ”¾åœ¨ from utils.dataset import UnionDataset ä¹‹å‰
try:
    import monai.transforms.transform

    # å¼ºåˆ¶ä¿®æ”¹ MONAI å†…éƒ¨çš„ MAX_SEEDï¼Œé˜²æ­¢ NumPy æŠ¥é”™ (OverflowError)
    monai.transforms.transform.MAX_SEED = 0xFFFFFFFF  # å³ 4294967295
except ImportError:
    pass
# ==========================================

import hydra
import torch
import torch.utils
from omegaconf import OmegaConf
from torch.utils.data import RandomSampler, Subset

from lightning.pytorch import seed_everything
from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint
from lightning.pytorch.loggers import WandbLogger, CSVLogger

from utils.dataset import UnionDataset
from utils.evaluation import Evaluator

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)


# åœ¨mainå‡½æ•°ä¹‹å‰å®šä¹‰è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜)
def _log_validation_details(phase, trainer, pl_module, dataset_name):
    """è®°å½•éªŒè¯è¯¦ç»†ç»“æœ"""
    # ä»…åœ¨ä¸»è¿›ç¨‹æ‰“å°
    if trainer.global_rank != 0:
        return

    current_metrics = trainer.callback_metrics
    dice_score = current_metrics.get(f"{dataset_name}_val_dice", None)
    val_dice_metric = current_metrics.get("val_DiceMetric", None)

    logger.info("ğŸ“ˆ " + "=" * 50)
    logger.info(f"ğŸ“‹ {phase}ç»“æœæ‘˜è¦")
    logger.info("ğŸ“ˆ " + "=" * 50)
    logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
    if dice_score is not None:
        logger.info(f"ğŸ¯ æ•°æ®é›†Dice: {dice_score:.4f}")
    if val_dice_metric is not None:
        logger.info(f"â­ ç»¼åˆDiceæŒ‡æ ‡: {val_dice_metric:.4f}")
    logger.info("ğŸ“ˆ " + "=" * 50)


def _log_test_summary(trainer, pl_module, dataset_name):
    """è®°å½•æµ‹è¯•ç»“æœæ‘˜è¦"""
    # ä»…åœ¨ä¸»è¿›ç¨‹æ‰“å°
    if trainer.global_rank != 0:
        return

    current_metrics = trainer.callback_metrics
    test_dice = current_metrics.get(f"{dataset_name}_test_dice", None)
    test_dice_metric = current_metrics.get("test_DiceMetric", None)

    logger.info("ğŸ‰ " + "=" * 60)
    logger.info("ğŸ† æœ€ç»ˆæµ‹è¯•ç»“æœæŠ¥å‘Š")
    logger.info("ğŸ‰ " + "=" * 60)
    if test_dice is not None:
        logger.info(f"âœ… æµ‹è¯•é›†Diceåˆ†æ•°: {test_dice:.4f}")
    if test_dice_metric is not None:
        logger.info(f"ğŸ… æœ€ç»ˆDiceæŒ‡æ ‡: {test_dice_metric:.4f}")
    logger.info("ğŸ‰ " + "=" * 60)


@hydra.main(config_path="../configs", config_name="mutil_train", version_base="1.3.2")
def main(cfg):
    """
    æ¨¡å‹çš„å¾®è°ƒä¸»å‡½æ•° (å·²é€‚é…å¤šå¡DDPè®­ç»ƒåŠåŠ¨æ€æ­¥æ•°è°ƒæ•´)
    """

    # è®¾ç½®éšæœºç§å­ç¡®ä¿å®éªŒå¯å¤ç°æ€§
    seed_everything(cfg.seed, True)
    # è®¾ç½®çŸ©é˜µä¹˜æ³•ç²¾åº¦å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
    torch.set_float32_matmul_precision("medium")

    # è·å–å½“å‰è¿›ç¨‹çš„å…¨å±€ rankï¼Œç”¨äºæ§åˆ¶æ—¥å¿—æ‰“å°
    global_rank = int(os.environ.get("RANK", 0))

    # æ„å»ºè¿è¡Œåç§°ï¼ŒåŒ…å«å…³é”®å®éªŒä¿¡æ¯
    dataset_name = list(cfg.data.keys())[0]  # è·å–æ•°æ®é›†åç§°
    # 1. è·å–å®Œæ•´è·¯å¾„å­—ç¬¦ä¸²
    full_data_path = cfg.data[dataset_name].path

    # 2. ä½¿ç”¨ Path å¯¹è±¡æå–æœ€åä¸€ä¸ªæ–‡ä»¶å¤¹å
    last_folder_name = os.path.basename(os.path.normpath(full_data_path))

    run_name = f'{cfg.loss_name}_{cfg.num_shots}shot_{last_folder_name}'

    # å¼ºåˆ¶è®¾ç½®ä¸ºç¦»çº¿æ¨¡å¼
    cfg.offline = True

    # ---------------------------------------------------------
    # è®¾ç½®æ—¥å¿—ä¿å­˜çš„ç»å¯¹è·¯å¾„ (ä»… Rank 0 åˆ›å»ºç›®å½•)
    # ---------------------------------------------------------
    save_root_dir = "/home/yangrui/Project/Base-models/local_results/doc/" + cfg.data_name
    if global_rank == 0:
        os.makedirs(save_root_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        logger.info(f"ğŸ“‚ æ—¥å¿—å­˜å‚¨è·¯å¾„å·²è®¾ç½®ä¸º: {save_root_dir}")

    # åˆå§‹åŒ–Weights & Biasesæ—¥å¿—è®°å½•å™¨ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
    wnb_logger = WandbLogger(
        save_dir=save_root_dir,
        project=cfg.wandb_project,
        name=run_name,
        config=OmegaConf.to_container(cfg),
        offline=True,
        mode="offline"
    )

    # åŒæ—¶æ·»åŠ CSVæ—¥å¿—è®°å½•å™¨
    csv_logger = CSVLogger(
        save_dir=save_root_dir,
        name=run_name,
        version="version_0"
    )

    # ---------------------------------------------------------
    # ã€æ ¸å¿ƒä¿®æ”¹ 1ã€‘æå‰è·å–è®¾å¤‡ä¿¡æ¯å¹¶è®¡ç®—åŠ¨æ€æ­¥æ•°
    # ---------------------------------------------------------
    # ç¡®å®šä½¿ç”¨çš„ GPU åˆ—è¡¨å’Œæ•°é‡
    target_devices = cfg.devices
    num_devices = len(target_devices)

    # è¯»å– yaml ä¸­çš„ max_steps (10000) ä½œä¸ºâ€œåŸºå‡†æ€»è®¡ç®—é‡â€
    base_total_steps = cfg.trainer.lightning_trainer.max_steps

    # é€»è¾‘ï¼šGPU è¶Šå¤šï¼Œå•å¡æ­¥æ•°è¶Šå°‘ï¼Œä¿æŒæ€» Batch é‡çº§ä¸€è‡´
    # 4å¡: 10000/4 = 2500 æ­¥; 2å¡: 10000/2 = 5000 æ­¥
    actual_max_steps = int(base_total_steps // num_devices)

    # é€»è¾‘ï¼šä¿æŒè¯„ä¼°å¯†åº¦ä¸€è‡´ã€‚æ€»æ­¥æ•°çš„ 1/25 è¿›è¡Œä¸€æ¬¡è¯„ä¼° (å…¨ç¨‹è¯„ä¼°çº¦25æ¬¡)
    # 4å¡: 2500/25 = 100æ­¥; 2å¡: 5000/25 = 200æ­¥
    actual_val_interval = int(actual_max_steps // 25)

    if global_rank == 0:
        logger.info("=" * 40)
        logger.info(f"ğŸ§® åŠ¨æ€è®­ç»ƒç­–ç•¥è°ƒæ•´ (GPUæ•°é‡: {num_devices})")
        logger.info("=" * 40)
        logger.info(f"   - YAMLåŸºå‡†æ­¥æ•°: {base_total_steps}")
        logger.info(f"   - å®é™…è®­ç»ƒæ­¥æ•° (max_steps): {actual_max_steps}")
        logger.info(f"   - è¯„ä¼°é—´éš” (val_check_interval): {actual_val_interval} steps")
        logger.info("=" * 40)

    # è®¾ç½®è®­ç»ƒå›è°ƒå‡½æ•°
    lr_monitor = LearningRateMonitor()
    monitor_metric = "val_DiceMetric"

    # è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼Œç”¨äºæ‰“å°éªŒè¯ç»“æœ (ValidationResultCallback) - ä¿æŒä¸å˜
    class ValidationResultCallback(LearningRateMonitor):
        def on_validation_end(self, trainer, pl_module):
            if trainer.global_rank != 0:
                return

            current_metrics = trainer.callback_metrics
            dice_score = current_metrics.get(f"{dataset_name}_val_dice", None)
            val_dice_metric = current_metrics.get("val_DiceMetric", None)
            val_loss = current_metrics.get(f"{dataset_name}_val_loss", None)

            current_step = trainer.global_step
            current_epoch = trainer.current_epoch

            logger.info("=" * 60)
            logger.info("ğŸ“Š éªŒè¯ç»“æœæŠ¥å‘Š")
            logger.info("=" * 60)
            logger.info(f"ğŸƒâ€â™‚ï¸ å½“å‰è®­ç»ƒè¿›åº¦: Epoch {current_epoch} | Step {current_step}")
            logger.info(f"ğŸ¯ æ•°æ®é›†: {dataset_name}")

            if dice_score is not None:
                logger.info(f"âœ… {dataset_name} Diceåˆ†æ•°: {dice_score:.4f}")
            if val_dice_metric is not None:
                logger.info(f"ğŸ† éªŒè¯DiceæŒ‡æ ‡: {val_dice_metric:.4f}")
            if val_loss is not None:
                logger.info(f"ğŸ“‰ éªŒè¯æŸå¤±å€¼: {val_loss:.4f}")

            if hasattr(trainer, 'checkpoint_callback') and trainer.checkpoint_callback is not None:
                best_dice = trainer.checkpoint_callback.best_model_score
                if best_dice is not None:
                    logger.info(f"â­ å†å²æœ€ä½³Dice: {best_dice:.4f}")
                    if val_dice_metric is not None:
                        improvement = val_dice_metric - best_dice
                        if improvement > 0:
                            logger.info(f"ğŸš€ ç›¸æ¯”æœ€ä½³æå‡: +{improvement:.4f}")
                        else:
                            logger.info(f"ğŸ“Œ è·ç¦»æœ€ä½³ç›¸å·®: {improvement:.4f}")

            logger.info("=" * 60)

    # ---------------------------------------------------------
    # ã€æ ¸å¿ƒä¿®æ”¹ 2ã€‘æƒé‡å‘½åä¸­åŠ å…¥ GPU æ•°é‡
    # ---------------------------------------------------------
    # filename æ ¼å¼ç¤ºä¾‹: step=2499_val_DiceMetric=0.85_4GPUs.ckpt
    checkpoint_callback = ModelCheckpoint(
        dirpath=cfg.chkpt_folder + "/" + cfg.data_name + "/" + last_folder_name + "/" + run_name,
        monitor=monitor_metric,
        save_top_k=1,
        mode="max",
        # ä¿®æ”¹è¿™é‡Œï¼šåœ¨æ–‡ä»¶åæœ«å°¾æ·»åŠ  _{num_devices}GPUs
        filename="{step}_{" + monitor_metric + ":.2f}_" + f"{num_devices}GPUs",
        auto_insert_metric_name=True,
        save_last=True
    )
    checkpoint_callback.CHECKPOINT_EQUALS_CHAR = ":"
    checkpoint_callback.CHECKPOINT_NAME_LAST = run_name + "_last"

    validation_callback = ValidationResultCallback()

    # å®ä¾‹åŒ– Trainer
    trainer_cls = hydra.utils.instantiate(cfg.trainer.lightning_trainer)

    # ---------------------------------------------------------
    # ã€æ ¸å¿ƒä¿®æ”¹ 3ã€‘ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„å‚æ•°è¦†ç›–é…ç½®
    # ---------------------------------------------------------
    trainer_additional_kwargs = {
        "logger": [wnb_logger, csv_logger],
        "callbacks": [lr_monitor, checkpoint_callback, validation_callback],

        # åŠ¨æ€è¦†ç›– yaml ä¸­çš„é…ç½®
        "max_steps": actual_max_steps,
        "val_check_interval": actual_val_interval,

        "devices": target_devices,
        "accelerator": "gpu",
        "strategy": "ddp",
        "sync_batchnorm": True,
        "use_distributed_sampler": False
    }

    trainer = trainer_cls(**trainer_additional_kwargs)


    # ---------------------------------------------------------
    # ã€æ•°æ®é‡‡æ ·å™¨è°ƒæ•´ã€‘é€‚é… UnionDataset æ ¼å¼ (æ ¸å¿ƒä¿®å¤éƒ¨åˆ†)
    # ---------------------------------------------------------

    # 0. å®šä¹‰ä¸€ä¸ªå†…éƒ¨ Dataset ç±»
    # ä½œç”¨ï¼š1. å­˜å‚¨åœ¨å†…å­˜ä¸­çš„ List æ•°æ®; 2. åƒ UnionDataset ä¸€æ ·è¿”å› (Tuple) è€Œä¸æ˜¯ (Dict)
    class FewShotInMemoryDataset(torch.utils.data.Dataset):
        def __init__(self, data_list, transform):
            self.data = data_list
            self.transform = transform

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            item = self.data[idx]
            # 1. åº”ç”¨ Transforms (MONAI Transforms è¾“å…¥ Dictï¼Œè¾“å‡º Dict)
            transformed = self.transform(item)

            # 2. ã€å…³é”®ä¿®å¤ã€‘å¼ºåˆ¶è§£åŒ…ä¸º Tupleï¼Œæ¨¡æ‹Ÿ UnionDataset çš„è¡Œä¸º
            # å¿…é¡»è¿”å› (Image, Mask) çš„å€¼ï¼Œè€Œä¸æ˜¯ keys (å­—ç¬¦ä¸²)
            return transformed['Image'], transformed['Mask'] > 0

    # 1. å®ä¾‹åŒ–åŸå§‹ Dataset è·å–é…ç½®ä¿¡æ¯ (Reader, Transforms ç­‰)
    raw_train_dataset = UnionDataset(cfg.data, "train", finetune=True)

    # è·å–å†…éƒ¨ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„ä¿¡æ¯
    dataset_info = raw_train_dataset.datasets[0]
    data_paths = dataset_info["paths"]
    reader = dataset_info["reader"]
    # æ³¨æ„ï¼šUnionDataset æ²¡æœ‰ .transform å±æ€§ï¼Œå˜æ¢å­˜å‚¨åœ¨ dataset_info å­—å…¸ä¸­
    data_transform = dataset_info["transforms"]

    subset_data_list = []

    # 2. æ‰‹åŠ¨é¢„åŠ è½½å‰ num_shots ä¸ªæ ·æœ¬çš„æ•°æ® (Image & Mask)
    # å¿…é¡»åœ¨è¿™é‡ŒåŠ è½½ï¼Œå› ä¸º UnionDataset çš„ transforms æœŸæœ›è¾“å…¥æ˜¯ Array è€Œä¸æ˜¯ Path
    shots_to_load = min(cfg.num_shots, len(data_paths))

    if global_rank == 0:
        logger.info(f"ğŸš€ æ­£åœ¨å°† {shots_to_load} ä¸ª Few-Shot æ ·æœ¬æ‰‹åŠ¨åŠ è½½åˆ°å†…å­˜ç¼“å­˜ä¸­...")

    # ä½¿ç”¨ç®€å•çš„å¾ªç¯è¯»å–æ•°æ®
    for i in range(shots_to_load):
        sample_path = data_paths[i]

        # å¤ç”¨ dataset.py ä¸­çš„æ–‡ä»¶æŸ¥æ‰¾é€»è¾‘
        img_path = [p for p in sample_path.iterdir() if 'img' in p.name][0]
        mask_path = [p for p in sample_path.iterdir() if 'label' in p.name][0]

        # å¤ç”¨ dataset.py ä¸­çš„è¯»å–é€»è¾‘ (è¯»å–ä¸º Numpy Array)
        img = reader.read_images(str(img_path))[0].astype(np.float32)
        mask = reader.read_images(str(mask_path))[0].astype(bool)

        # æ„å»ºç¬¦åˆ Transforms é¢„æœŸçš„å­—å…¸ (Keys å¿…é¡»åŒ¹é… dataset.py ä¸­çš„å®šä¹‰)
        subset_data_list.append({'Image': img, 'Mask': mask})

    if global_rank == 0:
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(subset_data_list)} ä¸ªæ ·æœ¬åˆ°å†…å­˜")

    # 3. ä½¿ç”¨è‡ªå®šä¹‰çš„ Dataset æ›¿ä»£ CacheDataset
    # è¿™ç¡®ä¿äº† __getitem__ è¿”å›çš„æ˜¯ tuple(tensor, tensor) è€Œä¸æ˜¯ dict
    train_dataset = FewShotInMemoryDataset(
        data_list=subset_data_list,
        transform=data_transform
    )

    # ---------------------------------------------------------
    # é‡‡æ ·å™¨é…ç½®
    # ---------------------------------------------------------
    # è®¡ç®—æ¯å¼ å¡éœ€è¦è·‘çš„æ ·æœ¬æ•°ï¼Œä¿æŒæ€» Epoch è§„æ¨¡ä¸å˜
    total_samples_per_epoch = int(1e5)
    samples_per_gpu = total_samples_per_epoch // num_devices

    if global_rank == 0:
        logger.info(f"Multi-GPU Config: {num_devices} GPUs ({target_devices})")
        logger.info(
            f"Sampler: {samples_per_gpu} samples per GPU (Total effective epoch size: {samples_per_gpu * num_devices})")

    # ä½¿ç”¨éšæœºé‡‡æ ·å™¨å¹¶è¿›è¡Œé‡å¤é‡‡æ ·
    random_sampler = RandomSampler(train_dataset, replacement=True, num_samples=samples_per_gpu)

    train_loader = hydra.utils.instantiate(cfg.dataloader)(
        dataset=train_dataset,
        sampler=random_sampler,
        # num_workers=4
    )

    # éªŒè¯å’Œæµ‹è¯•æ•°æ®é›† (ä¿æŒä¸å˜)
    val_dataset = UnionDataset(cfg.data, "val", finetune=True)
    val_loader = hydra.utils.instantiate(cfg.dataloader)(dataset=val_dataset, batch_size=1)
    if global_rank == 0: logger.info(f"Val dataset size: {len(val_dataset)}")

    test_dataset = UnionDataset(cfg.data, "test", finetune=True)
    test_loader = hydra.utils.instantiate(cfg.dataloader)(dataset=test_dataset, batch_size=1)
    if global_rank == 0: logger.info(f"Test dataset size: {len(test_dataset)}")

    # åˆå§‹åŒ–æ¨¡å‹ (ä¿æŒä¸å˜)
    model = hydra.utils.instantiate(cfg.model)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡ (ä¿æŒä¸å˜)
    if cfg.path_to_chkpt is not None:
        try:
            chkpt = torch.load(cfg.path_to_chkpt, map_location='cpu', weights_only=True)
        except:
            chkpt = torch.load(cfg.path_to_chkpt, map_location='cpu', weights_only=False)

        if isinstance(chkpt, dict):
            model_chkpt = chkpt.get('state_dict', chkpt.get('model_state_dict', chkpt.get('models', chkpt)))
        else:
            model_chkpt = chkpt

        if isinstance(model_chkpt, dict) and any(k.startswith('models.') for k in model_chkpt.keys()):
            from collections import OrderedDict
            model_chkpt = OrderedDict([(k.replace('models.', '', 1) if k.startswith('models.') else k, v)
                                       for k, v in model_chkpt.items()])

        model.load_state_dict(model_chkpt, strict=False)
        if global_rank == 0:
            logger.info(f"Loaded pretrained weights from {cfg.path_to_chkpt}")

    # åˆå§‹åŒ–Lightningæ¨¡å— (ä¿æŒä¸å˜)
    evaluator = Evaluator()
    lightning_module = hydra.utils.instantiate(cfg.trainer.lightning_module)(
        model=model,
        evaluator=evaluator,
        dataset_name=dataset_name
    )

    # è®­ç»ƒæµç¨‹ (ä¿æŒä¸å˜)
    if not cfg.offline:
        if global_rank == 0:
            wnb_logger.watch(model, log="all", log_freq=20)
    else:
        if global_rank == 0:
            logger.info("ç¦»çº¿æ¨¡å¼ï¼šè·³è¿‡æ¨¡å‹å‚æ•°ç›‘æ§")

        if cfg.num_shots == 0:
            if global_rank == 0: logger.info("Starting zero-shot evaluation")
            trainer.test(lightning_module, test_loader)
        else:
            if global_rank == 0:
                logger.info("Starting training")
                logger.info("ğŸ” è¿›è¡Œåˆå§‹éªŒè¯...")

            trainer.validate(lightning_module, val_loader)

            if global_rank == 0:
                _log_validation_details("åˆå§‹éªŒè¯", trainer, lightning_module, dataset_name)
                logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")

            trainer.fit(lightning_module, train_loader, val_loader)

            if global_rank == 0:
                logger.info("Finished training")
                logger.info("ğŸ§ª è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")

            trainer.test(lightning_module, test_loader, ckpt_path="best")

            if global_rank == 0:
                _log_test_summary(trainer, lightning_module, dataset_name)
                logger.info(f"å®éªŒå®Œæˆï¼æ—¥å¿—ä¿å­˜åœ¨ï¼š{save_root_dir}")


if __name__ == "__main__":
    # è®¾ç½®æ ‡å‡†è¾“å‡ºç¼“å†²ï¼Œç¡®ä¿æ—¥å¿—å®æ—¶æ˜¾ç¤º
    sys.stdout = open(sys.stdout.fileno(), mode="w", buffering=1)
    sys.stderr = open(sys.stderr.fileno(), mode="w", buffering=1)

    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢wandbè¯¢é—®
    import os

    os.environ["WANDB_SILENT"] = "true"
    os.environ["WANDB_MODE"] = "offline"

    main()


