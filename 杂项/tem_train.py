import logging
import sys
import warnings

# ==========================================
# ã€å…³é”®ä¿®å¤ã€‘MONAI ä¸ NumPy ç‰ˆæœ¬å…¼å®¹æ€§ä¿®å¤
# å¿…é¡»æ”¾åœ¨ from utils.dataset import UnionDataset ä¹‹å‰
try:
    import monai.transforms.transform
    # å¼ºåˆ¶ä¿®æ”¹ MONAI å†…éƒ¨çš„ MAX_SEEDï¼Œé˜²æ­¢ NumPy æŠ¥é”™ (OverflowError)
    monai.transforms.transform.MAX_SEED = 0xFFFFFFFF # å³ 4294967295
except ImportError:
    pass
# ==========================================
import hydra
import torch
import torch.utils
from omegaconf import OmegaConf
from torch.utils.data import RandomSampler, Subset

from lightning.pytorch import seed_everything
from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint
from lightning.pytorch.loggers import WandbLogger, CSVLogger

from utils.dataset import UnionDataset
from utils.evaluation import Evaluator

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)


# åœ¨mainå‡½æ•°ä¹‹å‰å®šä¹‰è¾…åŠ©å‡½æ•°
def _log_validation_details(phase, trainer, pl_module, dataset_name):
    """è®°å½•éªŒè¯è¯¦ç»†ç»“æœ"""
    current_metrics = trainer.callback_metrics
    dice_score = current_metrics.get(f"{dataset_name}_val_dice", None)
    val_dice_metric = current_metrics.get("val_DiceMetric", None)

    logger.info("ğŸ“ˆ " + "=" * 50)
    logger.info(f"ğŸ“‹ {phase}ç»“æœæ‘˜è¦")
    logger.info("ğŸ“ˆ " + "=" * 50)
    logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
    if dice_score is not None:
        logger.info(f"ğŸ¯ æ•°æ®é›†Dice: {dice_score:.4f}")
    if val_dice_metric is not None:
        logger.info(f"â­ ç»¼åˆDiceæŒ‡æ ‡: {val_dice_metric:.4f}")
    logger.info("ğŸ“ˆ " + "=" * 50)


def _log_test_summary(trainer, pl_module, dataset_name):
    """è®°å½•æµ‹è¯•ç»“æœæ‘˜è¦"""
    current_metrics = trainer.callback_metrics
    test_dice = current_metrics.get(f"{dataset_name}_test_dice", None)
    test_dice_metric = current_metrics.get("test_DiceMetric", None)

    logger.info("ğŸ‰ " + "=" * 60)
    logger.info("ğŸ† æœ€ç»ˆæµ‹è¯•ç»“æœæŠ¥å‘Š")
    logger.info("ğŸ‰ " + "=" * 60)
    if test_dice is not None:
        logger.info(f"âœ… æµ‹è¯•é›†Diceåˆ†æ•°: {test_dice:.4f}")
    if test_dice_metric is not None:
        logger.info(f"ğŸ… æœ€ç»ˆDiceæŒ‡æ ‡: {test_dice_metric:.4f}")
    logger.info("ğŸ‰ " + "=" * 60)


@hydra.main(config_path="../configs", config_name="tem_train", version_base="1.3.2")
def main(cfg):
    """
    æ¨¡å‹çš„å¾®è°ƒä¸»å‡½æ•°
    - é›¶æ ·æœ¬(zero-shot): num_shots=0ï¼Œç›´æ¥æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹
    - å•æ ·æœ¬(one-shot): num_shots=1ï¼Œä½¿ç”¨1ä¸ªæ ·æœ¬å¾®è°ƒ
    - å°‘æ ·æœ¬(few-shot): num_shots=3ï¼Œä½¿ç”¨3ä¸ªæ ·æœ¬å¾®è°ƒ
    """

    # è®¾ç½®éšæœºç§å­ç¡®ä¿å®éªŒå¯å¤ç°æ€§
    seed_everything(cfg.seed, True)
    # è®¾ç½®çŸ©é˜µä¹˜æ³•ç²¾åº¦å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
    torch.set_float32_matmul_precision("medium")

    # æ„å»ºè¿è¡Œåç§°ï¼ŒåŒ…å«å…³é”®å®éªŒä¿¡æ¯
    dataset_name = list(cfg.data.keys())[0]  # è·å–æ•°æ®é›†åç§°
    run_name = f'{cfg.num_shots}shot_{dataset_name}'

    # å¼ºåˆ¶è®¾ç½®ä¸ºç¦»çº¿æ¨¡å¼ï¼Œé¿å…è¯¢é—®ä¸Šä¼ ç½‘ç»œ
    cfg.offline = True

    # ---------------------------------------------------------
    # ã€ä¿®æ”¹ç‚¹ã€‘è®¾ç½®æ—¥å¿—ä¿å­˜çš„ç»å¯¹è·¯å¾„
    # ---------------------------------------------------------
    save_root_dir = "/home/yangrui/Project/Base-models/local_results/doc"
    os.makedirs(save_root_dir, exist_ok=True) # ç¡®ä¿ç›®å½•å­˜åœ¨
    logger.info(f"ğŸ“‚ æ—¥å¿—å­˜å‚¨è·¯å¾„å·²è®¾ç½®ä¸º: {save_root_dir}")

    # åˆå§‹åŒ–Weights & Biasesæ—¥å¿—è®°å½•å™¨ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
    wnb_logger = WandbLogger(
        save_dir=save_root_dir,     # <--- ä¿®æ”¹ï¼šæŒ‡å®šwandbä¿å­˜è·¯å¾„
        project=cfg.wandb_project,  # é¡¹ç›®åç§°
        name=run_name,              # è¿è¡Œåç§°
        config=OmegaConf.to_container(cfg),  # è®°å½•å®Œæ•´é…ç½®
        offline=True,               # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œä¸è¯¢é—®ä¸Šä¼ 
        mode="offline"              # æ˜ç¡®è®¾ç½®ä¸ºç¦»çº¿æ¨¡å¼
    )

    # åŒæ—¶æ·»åŠ CSVæ—¥å¿—è®°å½•å™¨ï¼Œç¡®ä¿æ—¥å¿—åœ¨æœ¬åœ°å­˜å‚¨
    csv_logger = CSVLogger(
        save_dir=save_root_dir,     # <--- ä¿®æ”¹ï¼šæŒ‡å®šCSVæ—¥å¿—ä¿å­˜è·¯å¾„
        name=run_name,              # è¿è¡Œåç§°
        version="version_0"         # ç‰ˆæœ¬å·
    )

    # è®¾ç½®è®­ç»ƒå›è°ƒå‡½æ•°
    lr_monitor = LearningRateMonitor()  # å­¦ä¹ ç‡ç›‘æ§
    monitor_metric = "val_DiceMetric"  # ç›‘æ§æŒ‡æ ‡ï¼ˆDiceç³»æ•°ï¼‰

    # è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼Œç”¨äºæ‰“å°éªŒè¯ç»“æœ
    class ValidationResultCallback(LearningRateMonitor):
        def on_validation_end(self, trainer, pl_module):
            # è·å–å½“å‰éªŒè¯æŒ‡æ ‡
            current_metrics = trainer.callback_metrics
            dice_score = current_metrics.get(f"{dataset_name}_val_dice", None)
            val_dice_metric = current_metrics.get("val_DiceMetric", None)
            val_loss = current_metrics.get(f"{dataset_name}_val_loss", None)

            # è·å–å½“å‰è®­ç»ƒæ­¥æ•°å’Œepoch
            current_step = trainer.global_step
            current_epoch = trainer.current_epoch

            # æ‰“å°è¯¦ç»†çš„éªŒè¯ç»“æœ
            logger.info("=" * 60)
            logger.info("ğŸ“Š éªŒè¯ç»“æœæŠ¥å‘Š")
            logger.info("=" * 60)
            logger.info(f"ğŸƒâ€â™‚ï¸ å½“å‰è®­ç»ƒè¿›åº¦: Epoch {current_epoch} | Step {current_step}")
            logger.info(f"ğŸ¯ æ•°æ®é›†: {dataset_name}")

            if dice_score is not None:
                logger.info(f"âœ… {dataset_name} Diceåˆ†æ•°: {dice_score:.4f}")
            if val_dice_metric is not None:
                logger.info(f"ğŸ† éªŒè¯DiceæŒ‡æ ‡: {val_dice_metric:.4f}")
            if val_loss is not None:
                logger.info(f"ğŸ“‰ éªŒè¯æŸå¤±å€¼: {val_loss:.4f}")

            # æ‰“å°æœ€ä½³æŒ‡æ ‡å¯¹æ¯”
            if hasattr(trainer, 'checkpoint_callback') and trainer.checkpoint_callback is not None:
                best_dice = trainer.checkpoint_callback.best_model_score
                if best_dice is not None:
                    logger.info(f"â­ å†å²æœ€ä½³Dice: {best_dice:.4f}")
                    if val_dice_metric is not None:
                        improvement = val_dice_metric - best_dice
                        if improvement > 0:
                            logger.info(f"ğŸš€ ç›¸æ¯”æœ€ä½³æå‡: +{improvement:.4f}")
                        else:
                            logger.info(f"ğŸ“Œ è·ç¦»æœ€ä½³ç›¸å·®: {improvement:.4f}")

            logger.info("=" * 60)

    # æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ - ä¿å­˜æœ€ä½³æ¨¡å‹
    checkpoint_callback = ModelCheckpoint(
        dirpath=cfg.chkpt_folder + "/" + cfg.wandb_project + "/" + run_name,  # ä¿å­˜è·¯å¾„
        monitor=monitor_metric,  # ç›‘æ§çš„æŒ‡æ ‡
        save_top_k=1,  # åªä¿å­˜æœ€å¥½çš„1ä¸ªæ¨¡å‹
        mode="max",  # æŒ‡æ ‡è¶Šå¤§è¶Šå¥½
        filename="{step}_{" + monitor_metric + ":.2f}",  # æ–‡ä»¶åæ ¼å¼
        auto_insert_metric_name=True,  # è‡ªåŠ¨æ’å…¥æŒ‡æ ‡å
        save_last=True  # åŒæ—¶ä¿å­˜æœ€åä¸€ä¸ªepochçš„æ¨¡å‹
    )
    checkpoint_callback.CHECKPOINT_EQUALS_CHAR = ":"
    checkpoint_callback.CHECKPOINT_NAME_LAST = run_name + "_last"

    # åˆå§‹åŒ–éªŒè¯ç»“æœå›è°ƒ
    validation_callback = ValidationResultCallback()

    # åˆå§‹åŒ–PyTorch Lightningè®­ç»ƒå™¨
    trainer = hydra.utils.instantiate(cfg.trainer.lightning_trainer)
    trainer_additional_kwargs = {
        "logger": [wnb_logger, csv_logger],  # ä½¿ç”¨å¤šä¸ªæ—¥å¿—è®°å½•å™¨
        "callbacks": [lr_monitor, checkpoint_callback, validation_callback],  # å›è°ƒå‡½æ•°
        "devices": cfg.devices  # è®­ç»ƒè®¾å¤‡
    }
    trainer = trainer(**trainer_additional_kwargs)

    # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ - å…³é”®éƒ¨åˆ†å¯¹åº”è®ºæ–‡ä¸­çš„å®éªŒè®¾ç½®
    # è®­ç»ƒæ•°æ®é›†ï¼šä½¿ç”¨UnionDatasetå¹¶é™åˆ¶æ ·æœ¬æ•°é‡
    train_dataset = UnionDataset(cfg.data, "fine-tuning", finetune=True)
    train_dataset = Subset(train_dataset, range(cfg.num_shots))  # é™åˆ¶æ ·æœ¬æ•°é‡

    # ä½¿ç”¨éšæœºé‡‡æ ·å™¨å¹¶è¿›è¡Œé‡å¤é‡‡æ ·ï¼Œæ¨¡æ‹Ÿè®ºæ–‡ä¸­çš„å°‘æ ·æœ¬è®¾ç½®
    # éšæœºé‡‡æ ·ä¸¤ä¸‡æ¬¡ï¼Œæ¯æ¬¡è®­ç»ƒä¸ºä¸¤ä¸‡ä¸ªæ ·æœ¬ï¼Œbatch sizeä¸º4ï¼Œå…±è®­ç»ƒ2500è½®æ¬¡ï¼Œæ¯500æ¬¡è¿›è¡Œä¸€æ¬¡éªŒè¯ï¼Œå…±éªŒè¯25æ¬¡
    random_sampler = RandomSampler(train_dataset, replacement=True, num_samples=int(1e4))
    train_loader = hydra.utils.instantiate(cfg.dataloader)(dataset=train_dataset, sampler=random_sampler)
    logger.info(f"Train dataset size mapped to {len(train_dataset)} samples")

    # éªŒè¯æ•°æ®é›†
    val_dataset = UnionDataset(cfg.data, "val", finetune=True)
    val_loader = hydra.utils.instantiate(cfg.dataloader)(dataset=val_dataset, batch_size=1)
    logger.info(f"Val dataset size: {len(val_dataset)}")

    # æµ‹è¯•æ•°æ®é›†
    test_dataset = UnionDataset(cfg.data, "test", finetune=True)
    test_loader = hydra.utils.instantiate(cfg.dataloader)(dataset=test_dataset, batch_size=1)
    logger.info(f"Test dataset size: {len(test_dataset)}")

    # åˆå§‹åŒ–æ¨¡å‹
    model = hydra.utils.instantiate(cfg.model)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæŒ‡å®šäº†æ£€æŸ¥ç‚¹è·¯å¾„ï¼‰
    if cfg.path_to_chkpt is not None:
        try:
            chkpt = torch.load(cfg.path_to_chkpt, map_location=f'cuda:{cfg.devices[0]}', weights_only=True)
        except:
            chkpt = torch.load(cfg.path_to_chkpt, map_location=f'cuda:{cfg.devices[0]}', weights_only=False)

        # å¤„ç†çŠ¶æ€å­—å…¸
        if isinstance(chkpt, dict):
            model_chkpt = chkpt.get('state_dict', chkpt.get('model_state_dict', chkpt.get('models', chkpt)))
        else:
            model_chkpt = chkpt

        # ç§»é™¤"models."å‰ç¼€ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if isinstance(model_chkpt, dict) and any(k.startswith('models.') for k in model_chkpt.keys()):
            from collections import OrderedDict
            model_chkpt = OrderedDict([(k.replace('models.', '', 1) if k.startswith('models.') else k, v)
                                       for k, v in model_chkpt.items()])

        model.load_state_dict(model_chkpt, strict=False)
        logger.info(f"Loaded pretrained weights from {cfg.path_to_chkpt}")


    # åˆå§‹åŒ–Lightningæ¨¡å— - å°è£…è®­ç»ƒé€»è¾‘
    evaluator = Evaluator()  # è¯„ä¼°å™¨ï¼Œç”¨äºè®¡ç®—Diceå’ŒclDiceæŒ‡æ ‡
    lightning_module = hydra.utils.instantiate(cfg.trainer.lightning_module)(
        model=model,
        evaluator=evaluator,
        dataset_name=dataset_name
    )

    # åªåœ¨åœ¨çº¿æ¨¡å¼ä¸‹ç›‘æ§æ¨¡å‹å‚æ•°ï¼ˆç¦»çº¿æ¨¡å¼ä¸‹è·³è¿‡ï¼‰
    if not cfg.offline:
        wnb_logger.watch(model, log="all", log_freq=20)  # ç›‘æ§æ¨¡å‹å‚æ•°
    else:
        logger.info("ç¦»çº¿æ¨¡å¼ï¼šè·³è¿‡æ¨¡å‹å‚æ•°ç›‘æ§")

        # æ ¹æ®æ ·æœ¬æ•°é‡é€‰æ‹©ä¸åŒçš„å®éªŒæ¨¡å¼
        if cfg.num_shots == 0:
            # é›¶æ ·æœ¬è¯„ä¼°ï¼šç›´æ¥æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹
            logger.info("Starting zero-shot evaluation")
            trainer.test(lightning_module, test_loader)
        else:
            # å°‘æ ·æœ¬å¾®è°ƒï¼šéªŒè¯â†’è®­ç»ƒâ†’æµ‹è¯•å®Œæ•´æµç¨‹
            logger.info("Starting training")

            # åˆå§‹éªŒè¯å¹¶æ‰“å°ç»“æœ
            logger.info("ğŸ” è¿›è¡Œåˆå§‹éªŒè¯...")
            initial_val_results = trainer.validate(lightning_module, val_loader)
            # è®°å½•è¯¦ç»†æ—¥å¿—
            _log_validation_details("åˆå§‹éªŒè¯", trainer, lightning_module, dataset_name)

            # å¼€å§‹è®­ç»ƒ
            logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
            trainer.fit(lightning_module, train_loader, val_loader)

            logger.info("Finished training")

            # æœ€ç»ˆæµ‹è¯•å¹¶æ‰“å°ç»“æœ
            logger.info("ğŸ§ª è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
            trainer.test(lightning_module, test_loader, ckpt_path="best")

            # æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦
            _log_test_summary(trainer, lightning_module, dataset_name)

        # è®°å½•å®éªŒå®Œæˆä¿¡æ¯
        logger.info(f"å®éªŒå®Œæˆï¼æ—¥å¿—ä¿å­˜åœ¨ï¼š{save_root_dir}")


if __name__ == "__main__":
    # è®¾ç½®æ ‡å‡†è¾“å‡ºç¼“å†²ï¼Œç¡®ä¿æ—¥å¿—å®æ—¶æ˜¾ç¤º
    sys.stdout = open(sys.stdout.fileno(), mode="w", buffering=1)
    sys.stderr = open(sys.stderr.fileno(), mode="w", buffering=1)

    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé˜²æ­¢wandbè¯¢é—®
    import os
    os.environ["WANDB_SILENT"] = "true"
    os.environ["WANDB_MODE"] = "offline"

    main()