"""
upsample_infer.py
åŸºäº tem_infer.py ä¿®æ”¹ã€‚
ä¿ç•™æ‰€æœ‰åŸæœ‰é…ç½®å’Œæµç¨‹ï¼Œä»…å¢åŠ ï¼š
1. æ¨ç†å‰å¼ºåˆ¶ä¸Šé‡‡æ · (inference_target_shape)ã€‚
2. æ¨ç†åä¸‹é‡‡æ ·å›åŸåˆ†è¾¨ç‡ã€‚
3. ä¿å­˜ä¸¤ä»½ç»“æœ (åŸå°ºå¯¸ + ä¸Šé‡‡æ ·å°ºå¯¸)ã€‚
"""
import logging
import warnings
from pathlib import Path
import math
import os
import csv
import sys
from datetime import datetime

import torch
import torch.nn.functional as F
import torch.multiprocessing as mp
import hydra
import numpy as np
from tqdm import tqdm
from monai.inferers import SlidingWindowInfererAdapt
from skimage.morphology import remove_small_objects
from skimage.measure import label, regionprops
from omegaconf import OmegaConf

# ä¿æŒåŸæœ‰çš„å¼•ç”¨ä¸å˜
from utils.dataset import generate_transforms
from utils.io import determine_reader_writer
from utils.evaluation import Evaluator, calculate_mean_metrics
# ç›´æ¥å¤ç”¨ tem_infer ä¸­çš„å‡½æ•°ï¼Œç¡®ä¿é€»è¾‘ä¸€è‡´
from tem_infer import (
    save_csv_report,
    load_model,
    get_paths_nested,
    resample, # TTAç”¨çš„resample
    keep_largest_vessels,
    keep_closest_vessels,
    auto_infer_paths
)

warnings.filterwarnings("ignore")
try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass

logger = logging.getLogger(__name__)


def resize_tensor_force(tensor, target_shape, mode="trilinear"):
    """
    å¼ºåˆ¶ç¼©æ”¾åˆ°æŒ‡å®šå°ºå¯¸ [D, H, W]
    ä¿®å¤: å¼ºåˆ¶å°† OmegaConf çš„ ListConfig è½¬æ¢ä¸ºæ ‡å‡†çš„ python list[int]
    """
    # æ— è®ºä¼ å…¥çš„æ˜¯ ListConfig, tuple è¿˜æ˜¯ listï¼Œå…ˆå¼ºåˆ¶è½¬ä¸º listï¼Œå†è½¬ int
    if target_shape is not None:
        target_shape = [int(x) for x in list(target_shape)]

    input_ndim = tensor.ndim
    # å¢åŠ ç»´åº¦ä»¥é€‚é… interpolate (éœ€è¦ B, C, D, H, W)
    if input_ndim == 3:  # D, H, W -> 1, 1, D, H, W
        tensor = tensor.unsqueeze(0).unsqueeze(0)
    elif input_ndim == 4:  # C, D, H, W -> 1, C, D, H, W
        tensor = tensor.unsqueeze(0)

    # trilinear æ’å€¼é€šå¸¸ align_corners=False
    align = False if mode != 'nearest' else None

    # æ‰§è¡Œæ’å€¼
    resized = F.interpolate(tensor, size=target_shape, mode=mode, align_corners=align)

    # è¿˜åŸç»´åº¦
    if input_ndim == 3:
        return resized.squeeze(0).squeeze(0)
    elif input_ndim == 4:
        return resized.squeeze(0)
    return resized

def run_upsample_inference_worker(rank, gpu_id, image_paths, mask_paths, cfg, return_dict):
    """
    ä¿®æ”¹åçš„ Workerï¼šæ”¯æŒä¸Šé‡‡æ · -> æ¨ç† -> ä¿å­˜ä¸­é—´ç»“æœ -> ä¸‹é‡‡æ · -> è¯„ä¼°
    """
    device = torch.device(f"cuda:{gpu_id}")

    # è®¾ç½®éšæœºç§å­ (ä¿æŒ tem_infer é€»è¾‘)
    np.random.seed(cfg.seed + rank)
    torch.manual_seed(cfg.seed + rank)
    torch.cuda.manual_seed_all(cfg.seed + rank)

    # åŠ è½½æ¨¡å‹
    try:
        model = load_model(cfg, device)
        model.to(device)
        model.eval()
    except Exception as e:
        print(f"[GPU {gpu_id}] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    transforms = generate_transforms(cfg.transforms_config)

    # ç¡®å®š I/O (ä¿æŒ tem_infer é€»è¾‘)
    if not image_paths: return
    first_name = image_paths[0].name
    if 'nii.gz' in first_name:
        rw_suffix = 'nii.gz'
        save_ext = '.nii.gz'
    else:
        rw_suffix = image_paths[0].suffix
        save_ext = image_paths[0].suffix

    image_reader_writer = determine_reader_writer(rw_suffix)()
    save_writer = determine_reader_writer(rw_suffix)()

    # æ»‘åŠ¨çª—å£æ¨æ–­å™¨ (ä¿æŒ tem_infer é€»è¾‘)
    inferer = SlidingWindowInfererAdapt(
        roi_size=cfg.patch_size,
        sw_batch_size=cfg.batch_size,
        overlap=cfg.overlap,
        mode=cfg.mode,
        sigma_scale=cfg.sigma_scale,
        padding_mode=cfg.padding_mode
    )

    local_metrics = {}

    # è¾“å‡ºè®¾ç½®
    save_predictions = False
    output_folder = None
    if cfg.output_folder and str(cfg.output_folder).lower() != "none":
        save_predictions = True
        output_folder = Path(cfg.output_folder)
        output_folder.mkdir(parents=True, exist_ok=True)

    desc = f"GPU {gpu_id}"
    iterator = tqdm(enumerate(image_paths), total=len(image_paths), desc=desc, position=rank, leave=True)

    # è·å–ç›®æ ‡å°ºå¯¸é…ç½®
    target_shape_cfg = cfg.get("inference_target_shape", None)

    with torch.no_grad():
        for idx, image_path in iterator:
            preds = []
            try:
                img_data = image_reader_writer.read_images(image_path)[0].astype(np.float32)
            except Exception as e:
                print(f"[GPU {gpu_id}] è¯»å–å¤±è´¥ {image_path}: {e}")
                continue

            # 1. åŸºç¡€é¢„å¤„ç†
            image_tensor = transforms(img_data) # (C, D, H, W)
            if isinstance(image_tensor, np.ndarray):
                image_tensor = torch.from_numpy(image_tensor)
            if image_tensor.ndim == 3:
                image_tensor = image_tensor.unsqueeze(0)

            # --- ã€ä¿®æ”¹ç‚¹ 1ã€‘è®°å½•åŸå§‹å°ºå¯¸å¹¶å¼ºåˆ¶ä¸Šé‡‡æ · ---
            original_spatial_shape = image_tensor.shape[1:] # D, H, W

            # å¦‚æœé…ç½®äº† target_shapeï¼Œå¼ºåˆ¶ Resize è¾“å…¥å›¾åƒ
            if target_shape_cfg:
                image_tensor = resize_tensor_force(image_tensor, target_shape_cfg, mode="trilinear")

            # ç§»è‡³ GPU
            image_base = image_tensor.unsqueeze(0).to(device) # (1, C, D', H', W')

            # --- TTA & æ¨ç† (ä¿æŒ tem_infer é€»è¾‘ï¼Œä½†åœ¨ä¸Šé‡‡æ ·åçš„å›¾ä¸Šè¿›è¡Œ) ---
            for scale in cfg.tta.scales:
                image = image_base.clone()

                if cfg.tta.invert and image.mean() > cfg.tta.invert_mean_thresh:
                    image = 1 - image

                # TTA çš„ resample ä¹Ÿæ˜¯åŸºäºå½“å‰çš„ image_base å°ºå¯¸
                current_shape = image.shape # (1, C, D', H', W')
                image = resample(image, factor=scale)

                logits = inferer(image, model)
                logits = resample(logits, target_shape=current_shape)
                preds.append(logits.cpu().squeeze())

            # --- èåˆ ---
            if len(preds) > 1:
                stacked_preds = torch.stack(preds)
                if cfg.merging.max:
                    pred = stacked_preds.max(dim=0)[0].sigmoid()
                else:
                    pred = stacked_preds.mean(dim=0).sigmoid()
            else:
                pred = preds[0].sigmoid()

            # pred ç°åœ¨æ˜¯ Tensor (C, D', H', W')ï¼Œæ˜¯ä¸Šé‡‡æ ·åçš„æ¦‚ç‡å›¾

            # --- ã€ä¿®æ”¹ç‚¹ 2ã€‘ä¿å­˜ä¸Šé‡‡æ ·ç»“æœ & ä¸‹é‡‡æ ·å›åŸå°ºå¯¸ ---

            # A. å¤„ç†ä¸Šé‡‡æ ·ç»“æœ (ç”¨äºä¿å­˜)
            if save_predictions and target_shape_cfg:
                # äºŒå€¼åŒ–
                pred_upsampled_thresh = (pred > cfg.merging.threshold).numpy().astype(np.uint8)

                # ä¿å­˜æ–‡ä»¶ååŠ  _upsampled
                clean_name = image_path.name.replace('.img.nii.gz', '').replace('.nii.gz', '')
                save_name_up = f"{clean_name}_upsampled_{cfg.file_app}pred{save_ext}"
                save_writer.write_seg(pred_upsampled_thresh, output_folder / save_name_up)

            # B. ä¸‹é‡‡æ ·å›åŸå§‹å°ºå¯¸ (ç”¨äºåç»­æ ‡å‡†çš„ä¿å­˜å’Œè¯„ä¼°)
            if target_shape_cfg:
                # æ³¨æ„ï¼šå¯¹æ¦‚ç‡å›¾æ’å€¼ï¼Œä½¿ç”¨ trilinear
                pred = resize_tensor_force(pred, original_spatial_shape, mode="trilinear")

            # --- ä»¥ä¸‹å®Œå…¨ä¿æŒ tem_infer.py çš„é€»è¾‘ ---

            pred_thresh = (pred > cfg.merging.threshold).numpy()

            # --- åå¤„ç† (ä¿æŒä¸å˜) ---
            if cfg.post.apply:
                pred_thresh = remove_small_objects(
                    pred_thresh.astype(bool),
                    min_size=cfg.post.small_objects_min_size,
                    connectivity=cfg.post.small_objects_connectivity
                )
            if cfg.post.get('keep_largest_vessels', False):
                pred_thresh = keep_largest_vessels(pred_thresh.astype(int), cfg.post.num_largest_vessels)
            if cfg.post.get('keep_closest_vessels', False):
                pred_thresh = keep_closest_vessels(pred_thresh.astype(int), cfg.post.num_closest_vessels)

            # --- ä¿å­˜ (åŸå§‹å°ºå¯¸) ---
            if save_predictions:
                clean_name = image_path.name.replace('.img.nii.gz', '').replace('.nii.gz', '')
                save_name = f"{clean_name}_{cfg.file_app}pred{save_ext}"
                save_path = output_folder / save_name
                save_writer.write_seg(pred_thresh.astype(np.uint8), save_path)

            # --- è®¡ç®—æŒ‡æ ‡ (ä¿æŒä¸å˜) ---
            if mask_paths:
                if mask_paths[idx] is not None:
                    # è¯»å–åŸå§‹ GT (åŸå§‹å°ºå¯¸)
                    mask_data = image_reader_writer.read_images(mask_paths[idx])[0]
                    mask_tensor = torch.tensor(mask_data).bool().to(device)

                    # æ­¤æ—¶ pred_thresh å·²ç»è¢« resize å›åŸå§‹å°ºå¯¸äº†ï¼Œå¯ä»¥ç›´æ¥è®¡ç®—
                    post_processed_tensor = torch.from_numpy(pred_thresh).float().to(device)

                    metrics = Evaluator().estimate_metrics(post_processed_tensor, mask_tensor, threshold=0.5)

                    metrics_val = {k: v.item() if hasattr(v, 'item') else v for k, v in metrics.items()}
                    local_metrics[image_path.name] = metrics_val

                    dice_val = metrics_val.get('dice', 0)
                    cldice_val = metrics_val.get('cldice', metrics_val.get('clDice', 0))

                    msg = f"[GPU {gpu_id}] {image_path.name} | Dice: {dice_val:.4f} | clDice: {cldice_val:.4f}"
                    iterator.write(msg)

    # å­˜å…¥ç»“æœ
    return_dict[rank] = local_metrics


@hydra.main(config_path="../configs", config_name="upsample_infer", version_base="1.3.2")
def main(cfg):
    # å®Œå…¨å¤ç”¨ tem_infer çš„ main é€»è¾‘ï¼Œé™¤äº† worker å‡½æ•°å˜äº†

    cfg = auto_infer_paths(cfg)
    all_image_paths, all_mask_paths = get_paths_nested(cfg)
    total_samples = len(all_image_paths)

    try:
        dataset_name = Path(cfg.image_path).parent.name
    except:
        dataset_name = "Dataset"

    logger.info("=" * 80)
    logger.info(f"ğŸš€ å¼€å§‹ [ä¸Šé‡‡æ ·] æ¨ç†ä»»åŠ¡ (Dataset: {dataset_name})")
    # æ‰“å°ä¸€ä¸‹ç›®æ ‡å°ºå¯¸
    logger.info(f"ğŸ¯ å¼ºåˆ¶æ¨ç†å°ºå¯¸: {cfg.get('inference_target_shape', 'Disabled')}")
    logger.info(f"ğŸ“‚ æƒé‡è·¯å¾„: {cfg.ckpt_path}")
    logger.info("=" * 80)

    if not cfg.get("gpus"):
        target_gpus = [0]
    else:
        target_gpus = list(cfg.gpus)

    num_gpus = len(target_gpus)
    chunk_size = math.ceil(total_samples / num_gpus)
    chunks_img = [all_image_paths[i:i + chunk_size] for i in range(0, total_samples, chunk_size)]

    if all_mask_paths:
        chunks_mask = [all_mask_paths[i:i + chunk_size] for i in range(0, total_samples, chunk_size)]
    else:
        chunks_mask = [None] * len(chunks_img)

    manager = mp.Manager()
    return_dict = manager.dict()
    processes = []

    for rank, gpu_id in enumerate(target_gpus):
        if rank >= len(chunks_img):
            break

        # ä½¿ç”¨æ–°çš„ worker
        p = mp.Process(
            target=run_upsample_inference_worker,
            args=(rank, gpu_id, chunks_img[rank], chunks_mask[rank], cfg, return_dict)
        )
        p.start()
        processes.append(p)

    for p in processes:
        p.join()

    logger.info("æ‰€æœ‰è¿›ç¨‹å·²å®Œæˆï¼Œæ­£åœ¨æ±‡æ€»æŒ‡æ ‡...")
    final_metrics_dict = {}
    for rank, metrics in return_dict.items():
        final_metrics_dict.update(metrics)

    if final_metrics_dict:
        mean_metrics = calculate_mean_metrics(list(final_metrics_dict.values()), round_to=cfg.round_to)

        logger.info("=" * 60)
        logger.info(f"ğŸ FINAL METRICS ({len(final_metrics_dict)} cases):")
        for key in sorted(mean_metrics.keys()):
            val = mean_metrics[key]
            val = val.item() if hasattr(val, 'item') else val
            logger.info(f"Mean {key:<25}: {val:.4f}")
        logger.info("=" * 60)

        # å¤ç”¨ tem_infer çš„ä¿å­˜æŠ¥å‘Šå‡½æ•°
        save_csv_report(final_metrics_dict, mean_metrics, cfg, dataset_name)
    else:
        logger.info("æ²¡æœ‰äº§ç”Ÿè¯„ä¼°æŒ‡æ ‡")

if __name__ == "__main__":
    main()