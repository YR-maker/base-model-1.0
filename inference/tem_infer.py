"""
å¤šGPUæ¨¡å‹æ¨ç†è„šæœ¬ (æ”¯æŒ clDice æ‰“å°ä¸è®°å½•)
åŠŸèƒ½:
1. è‡ªåŠ¨åˆ‡åˆ†æ•°æ®åˆ°å¤šå¡å¹¶è¡Œæ¨ç†ã€‚
2. æ±‡æ€»ç»“æœå¹¶åœ¨æ§åˆ¶å°æ‰“å° (æ–°å¢ clDice æ˜¾ç¤º)ã€‚
3. ç”Ÿæˆæ ‡å‡†åŒ–çš„ CSV å®éªŒæŠ¥å‘Šï¼ŒåŒ…å«é…ç½®å…ƒæ•°æ®ã€å•ä¾‹å¾—åˆ†åŠæœ€ç»ˆå¹³å‡åˆ†ã€‚
4. æŠ¥å‘Šç»Ÿä¸€ä¿å­˜åœ¨è„šæœ¬è¿è¡Œç›®å½•ä¸‹çš„ local_results/tem_infer/{dataset_name} æ–‡ä»¶å¤¹ä¸­ã€‚
"""
import logging
import warnings
from pathlib import Path
import math
import os
import csv
import sys
from datetime import datetime

import torch
import torch.nn.functional as F
import torch.multiprocessing as mp
import hydra
import numpy as np
from tqdm import tqdm
from monai.inferers import SlidingWindowInfererAdapt
from skimage.morphology import remove_small_objects
from skimage.measure import label, regionprops
from omegaconf import OmegaConf

# ä¿æŒåŸæœ‰çš„å¼•ç”¨ä¸å˜
from utils.dataset import generate_transforms
from utils.io import determine_reader_writer
from utils.evaluation import Evaluator, calculate_mean_metrics

warnings.filterwarnings("ignore")
# é…ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹å¼ï¼ŒCUDAå¿…é¡»ä½¿ç”¨ spawn
try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass

logger = logging.getLogger(__name__)


def save_csv_report(final_metrics_dict, mean_metrics, cfg, dataset_name):
    """
    ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç”Ÿæˆæ ‡å‡†åŒ–çš„å®éªŒæŠ¥å‘Š CSV
    ä¿å­˜ä½ç½®: {åŸå§‹è¿è¡Œç›®å½•}/local_results/tem_infer/{dataset_name}/
    å‘½åæ ¼å¼: tem_infer_{æ•°æ®é›†}_{æ—¶é—´}.csv
    å†…å®¹ç»“æ„: é…ç½®ä¿¡æ¯ -> è¯¦ç»†æ•°æ® -> å¹³å‡æŒ‡æ ‡
    """
    # 1. ç¡®å®šä¿å­˜è·¯å¾„
    try:
        project_root = Path(hydra.utils.get_original_cwd())
    except:
        project_root = Path.cwd()

    csv_dir = project_root / "local_results" / "tem_infer" / dataset_name
    csv_dir.mkdir(parents=True, exist_ok=True)

    # 2. ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d")
    d_name = dataset_name if dataset_name else "UnknownData"
    filename = f"tem_infer_{d_name}_{timestamp}_{cfg.shot_name}shot.csv"
    save_path = csv_dir / filename

    # 3. å‡†å¤‡æ•°æ®
    sorted_keys = sorted(final_metrics_dict.keys())
    metric_names = []
    if sorted_keys:
        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ‰€æœ‰æŒ‡æ ‡åç§° (ä¾‹å¦‚: ['dice', 'cldice', 'iou'])
        metric_names = list(final_metrics_dict[sorted_keys[0]].keys())
        # å°è¯•å°† dice å’Œ cldice æ’åœ¨å‰é¢ï¼Œæ–¹ä¾¿æŸ¥çœ‹
        priority_keys = ['dice', 'cldice', 'clDice', 'iou']
        metric_names.sort(key=lambda x: (priority_keys.index(x) if x in priority_keys else 999, x))

    try:
        with open(save_path, mode='w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # --- Part 1: å®éªŒé…ç½®å…ƒæ•°æ® (Metadata) ---
            writer.writerow(["### Experiment Configuration Record ###"])
            writer.writerow(["Script Name", "tem_infer.py"])
            writer.writerow(["Date", timestamp])
            writer.writerow(["Dataset Name", d_name])
            writer.writerow(["Model Checkpoint", cfg.ckpt_path])
            writer.writerow(["Input Data Path", cfg.image_path])
            writer.writerow(["Output Mask Path", cfg.output_folder])
            writer.writerow(["TTA Scales", str(cfg.tta.scales)])
            writer.writerow(["TTA Invert", f"{cfg.tta.invert} (Thresh: {cfg.tta.invert_mean_thresh})"])
            writer.writerow(["Patch / Batch", f"{cfg.patch_size} / {cfg.batch_size}"])
            writer.writerow(["Merging Strategy", f"Max: {cfg.merging.max}, Thresh: {cfg.merging.threshold}"])

            post_str = f"SmallObj:{cfg.post.small_objects_min_size}" if cfg.post.apply else "None"
            if cfg.post.get('keep_largest_vessels'): post_str += f", KeepLargest:{cfg.post.num_largest_vessels}"
            writer.writerow(["Post Processing", post_str])

            writer.writerow([]) # ç©ºè¡Œåˆ†éš”

            # --- Part 2: è¯¦ç»†å¾—åˆ† (Detailed Scores) ---
            writer.writerow(["### Detailed Metrics per Case ###"])
            if sorted_keys:
                # è¡¨å¤´
                headers = ["Case Name"] + metric_names
                writer.writerow(headers)

                # æ•°æ®è¡Œ
                for name in sorted_keys:
                    row = [name] + [final_metrics_dict[name].get(k, "") for k in metric_names]
                    writer.writerow(row)
            else:
                writer.writerow(["No metrics calculated (Missing masks?)"])

            writer.writerow([]) # ç©ºè¡Œåˆ†éš”

            # --- Part 3: å¹³å‡æŒ‡æ ‡å¤§é›†åˆ (Aggregated Metrics) ---
            writer.writerow(["### Final Aggregated Metrics ###"])
            if mean_metrics:
                # å†™å…¥ä¸¤è¡Œï¼šä¸€è¡Œæ˜¯æŒ‡æ ‡åï¼Œä¸€è¡Œæ˜¯å¹³å‡å€¼
                # æŒ‰ç…§ metric_names çš„é¡ºåºå†™å…¥å¹³å‡å€¼
                sorted_mean_keys = [k for k in metric_names if k in mean_metrics]
                # åŠ ä¸ŠåŸæœ¬åœ¨ mean_metrics ä½†ä¸åœ¨ metric_names é‡Œçš„å…¶ä»–é”®
                for k in mean_metrics.keys():
                    if k not in sorted_mean_keys:
                        sorted_mean_keys.append(k)

                writer.writerow(["Metric"] + sorted_mean_keys)
                writer.writerow(["Average"] + [mean_metrics.get(k, 0) for k in sorted_mean_keys])

        logger.info(f"âœ… å®éªŒæŠ¥å‘Šå·²ä¿å­˜è‡³: {save_path}")
        logger.info(f"   (åŒ…å«äº†å®éªŒé…ç½®ã€{len(sorted_keys)}ä¸ªæ ·æœ¬çš„è¯¦ç»†å¾—åˆ†ä»¥åŠæœ€ç»ˆå¹³å‡å€¼)")

    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ CSV æŠ¥å‘Šå¤±è´¥: {e}")


def load_model(cfg, device):
    """åŠ è½½æ¨¡å‹æƒé‡"""
    ckpt_path = Path(cfg.ckpt_path)
    if not ckpt_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {ckpt_path}")

    try:
        ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)
    except Exception as e:
        raise RuntimeError(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")

    model = hydra.utils.instantiate(cfg.model)
    state_dict_to_load = None

    if 'state_dict' in ckpt:
        state_dict = ckpt['state_dict']
        new_state_dict = {}
        for key, value in state_dict.items():
            key = key.replace('model.', '').replace('models.', '').replace('net.', '').replace('module.', '')
            new_state_dict[key] = value
        state_dict_to_load = new_state_dict
    elif isinstance(ckpt, dict) and any(key.startswith(('encoder', 'decoder', 'backbone')) for key in ckpt.keys()):
        state_dict_to_load = ckpt
    else:
        state_dict_to_load = ckpt

    if state_dict_to_load is not None:
        try:
            model.load_state_dict(state_dict_to_load, strict=True)
        except RuntimeError:
            model.load_state_dict(state_dict_to_load, strict=False)
    else:
        raise ValueError("æƒé‡æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ")

    return model


def get_paths_nested(cfg):
    """è·å–æ‰€æœ‰æ–‡ä»¶è·¯å¾„"""
    root_dir = Path(hydra.utils.to_absolute_path(cfg.image_path))
    if not root_dir.exists():
        raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {root_dir}")

    image_paths = []
    mask_paths = []

    case_dirs = sorted([d for d in root_dir.iterdir() if d.is_dir()])
    logger.info(f"åœ¨ {root_dir} ä¸­æ‰¾åˆ° {len(case_dirs)} ä¸ªå­æ–‡ä»¶å¤¹")

    for case_dir in case_dirs:
        case_id = case_dir.name
        img_name = f"{case_id}.img.nii.gz"
        img_p = case_dir / img_name

        if img_p.exists():
            image_paths.append(img_p)
            if cfg.get('mask_suffix') and cfg.get('mask_path'):
                label_name = f"{case_id}{cfg.mask_suffix}"
                label_p = case_dir / label_name
                if label_p.exists():
                    mask_paths.append(label_p)
                elif cfg.get('strict_matching', False):
                    logger.warning(f"Case {case_id}: æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶ {label_name}")

    if not image_paths:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°ç¬¦åˆè§„åˆ™çš„å›¾åƒã€‚")

    if mask_paths and len(mask_paths) != len(image_paths):
        if cfg.get('strict_matching', True):
            raise ValueError("æ ‡ç­¾æ•°é‡ä¸åŒ¹é…")
        else:
            mask_paths = None

    if not mask_paths:
        return image_paths, None

    return image_paths, mask_paths


def resample(image, factor=None, target_shape=None):
    if factor == 1: return image
    if target_shape:
        _, _, new_d, new_h, new_w = target_shape
    else:
        _, _, d, h, w = image.shape
        new_d, new_h, new_w = int(round(d / factor)), int(round(h / factor)), int(round(w / factor))
    return F.interpolate(image, size=(new_d, new_h, new_w), mode="trilinear", align_corners=False)


# --- åå¤„ç†è¾…åŠ©å‡½æ•° ---
def keep_largest_vessels(prediction, num_vessels=3):
    if isinstance(prediction, torch.Tensor): prediction = prediction.cpu().numpy()
    prediction = prediction.astype(int)
    labeled_mask = label(prediction, connectivity=3)
    regions = regionprops(labeled_mask)
    if len(regions) <= num_vessels: return prediction
    regions_sorted = sorted(regions, key=lambda x: x.area, reverse=True)
    processed_mask = np.zeros_like(prediction)
    for i in range(min(num_vessels, len(regions_sorted))):
        coords = regions_sorted[i].coords
        processed_mask[coords[:, 0], coords[:, 1], coords[:, 2]] = 1
    return processed_mask

def keep_closest_vessels(prediction, num_vessels=3):
    if isinstance(prediction, torch.Tensor): prediction = prediction.cpu().numpy()
    prediction = prediction.astype(int)
    labeled_mask = label(prediction, connectivity=3)
    regions = regionprops(labeled_mask)
    if len(regions) <= num_vessels: return prediction
    image_center = np.array(prediction.shape) / 2.0
    regions_sorted = sorted(regions, key=lambda x: np.linalg.norm(np.array(x.centroid) - image_center))
    processed_mask = np.zeros_like(prediction)
    for i in range(min(num_vessels, len(regions_sorted))):
        coords = regions_sorted[i].coords
        processed_mask[coords[:, 0], coords[:, 1], coords[:, 2]] = 1
    return processed_mask


def run_inference_worker(rank, gpu_id, image_paths, mask_paths, cfg, return_dict):
    """
    å•ä¸ª Worker è¿›ç¨‹ï¼šè´Ÿè´£åœ¨ä¸€ä¸ª GPU ä¸Šè·‘ä¸€éƒ¨åˆ†æ•°æ®
    """
    device = torch.device(f"cuda:{gpu_id}")

    # è®¾ç½®éšæœºç§å­
    np.random.seed(cfg.seed + rank)
    torch.manual_seed(cfg.seed + rank)
    torch.cuda.manual_seed_all(cfg.seed + rank)

    # åŠ è½½æ¨¡å‹
    try:
        model = load_model(cfg, device)
        model.to(device)
        model.eval()
    except Exception as e:
        print(f"[GPU {gpu_id}] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    transforms = generate_transforms(cfg.transforms_config)

    # ç¡®å®š I/O
    first_name = image_paths[0].name
    if 'nii.gz' in first_name:
        rw_suffix = 'nii.gz'
        save_ext = '.nii.gz'
    else:
        rw_suffix = image_paths[0].suffix
        save_ext = image_paths[0].suffix

    image_reader_writer = determine_reader_writer(rw_suffix)()
    save_writer = determine_reader_writer(rw_suffix)()

    inferer = SlidingWindowInfererAdapt(
        roi_size=cfg.patch_size,
        sw_batch_size=cfg.batch_size,
        overlap=cfg.overlap,
        mode=cfg.mode,
        sigma_scale=cfg.sigma_scale,
        padding_mode=cfg.padding_mode
    )

    local_metrics = {}

    # è¾“å‡ºè®¾ç½®
    save_predictions = False
    output_folder = None
    if cfg.output_folder and str(cfg.output_folder).lower() != "none":
        save_predictions = True
        output_folder = Path(cfg.output_folder)
        output_folder.mkdir(parents=True, exist_ok=True)

    # è¿›åº¦æ¡
    desc = f"GPU {gpu_id}"
    iterator = tqdm(enumerate(image_paths), total=len(image_paths), desc=desc, position=rank, leave=True)

    with torch.no_grad():
        for idx, image_path in iterator:
            preds = []
            try:
                img_data = image_reader_writer.read_images(image_path)[0].astype(np.float32)
            except Exception as e:
                print(f"[GPU {gpu_id}] è¯»å–å¤±è´¥ {image_path}: {e}")
                continue

            # --- TTA & æ¨ç† ---
            for scale in cfg.tta.scales:
                image_tensor = transforms(img_data)
                if isinstance(image_tensor, np.ndarray):
                    image_tensor = torch.from_numpy(image_tensor)
                if image_tensor.ndim == 3:
                    image_tensor = image_tensor.unsqueeze(0)
                image = image_tensor.unsqueeze(0).to(device)

                if cfg.tta.invert and image.mean() > cfg.tta.invert_mean_thresh:
                    image = 1 - image

                original_shape = image.shape
                image = resample(image, factor=scale)

                logits = inferer(image, model)
                logits = resample(logits, target_shape=original_shape)
                preds.append(logits.cpu().squeeze())

            # --- èåˆ ---
            if len(preds) > 1:
                stacked_preds = torch.stack(preds)
                if cfg.merging.max:
                    pred = stacked_preds.max(dim=0)[0].sigmoid()
                else:
                    pred = stacked_preds.mean(dim=0).sigmoid()
            else:
                pred = preds[0].sigmoid()

            pred_thresh = (pred > cfg.merging.threshold).numpy()

            # --- åå¤„ç† ---
            if cfg.post.apply:
                pred_thresh = remove_small_objects(
                    pred_thresh.astype(bool),
                    min_size=cfg.post.small_objects_min_size,
                    connectivity=cfg.post.small_objects_connectivity
                )
            if cfg.post.get('keep_largest_vessels', False):
                pred_thresh = keep_largest_vessels(pred_thresh.astype(int), cfg.post.num_largest_vessels)
            if cfg.post.get('keep_closest_vessels', False):
                pred_thresh = keep_closest_vessels(pred_thresh.astype(int), cfg.post.num_closest_vessels)

            # --- ä¿å­˜ ---
            if save_predictions:
                clean_name = image_path.name.replace('.img.nii.gz', '').replace('.nii.gz', '')
                save_name = f"{clean_name}_{cfg.file_app}pred{save_ext}"
                save_path = output_folder / save_name
                save_writer.write_seg(pred_thresh.astype(np.uint8), save_path)

            # --- è®¡ç®—æŒ‡æ ‡ ---
            if mask_paths:
                if mask_paths[idx] is not None:
                    mask_data = image_reader_writer.read_images(mask_paths[idx])[0]
                    mask_tensor = torch.tensor(mask_data).bool().to(device)

                    post_processed_tensor = torch.from_numpy(pred_thresh).float().to(device)

                    metrics = Evaluator().estimate_metrics(post_processed_tensor, mask_tensor, threshold=0.5)

                    metrics_val = {k: v.item() if hasattr(v, 'item') else v for k, v in metrics.items()}
                    local_metrics[image_path.name] = metrics_val

                    # ã€ä¿®æ”¹ç‚¹ã€‘æ–°å¢æ‰“å° clDice
                    # ä¼˜å…ˆè·å– cldice æˆ– clDiceï¼Œå¦‚æœéƒ½æ²¡æœ‰åˆ™ä¸º 0
                    dice_val = metrics_val.get('dice', 0)
                    cldice_val = metrics_val.get('cldice', metrics_val.get('clDice', 0))

                    msg = f"[GPU {gpu_id}] {image_path.name} | Dice: {dice_val:.4f} | clDice: {cldice_val:.4f}"
                    iterator.write(msg)

    # å­˜å…¥ç»“æœ
    return_dict[rank] = local_metrics


import re
from pathlib import Path


def auto_infer_paths(cfg):
    """
    è‡ªåŠ¨è·¯å¾„æ¨å¯¼é€»è¾‘ï¼š
    1. å¦‚æœ image_path æˆ– output_folder æ˜¯ "auto" æˆ– Noneï¼Œåˆ™æ ¹æ® ckpt_path æ¨å¯¼ã€‚
    2. å¦‚æœæ˜¯å…·ä½“è·¯å¾„ï¼Œåˆ™ä¿ç•™åŸå€¼ï¼Œä¸ä¿®æ”¹ã€‚
    """
    # è·å–å½“å‰é…ç½®çš„å€¼ (è½¬ä¸ºå­—ç¬¦ä¸²å¹¶å°å†™ï¼Œé˜²æ­¢å†™æˆ "Auto")
    raw_img_path = str(cfg.get("image_path", "auto")).strip()
    raw_out_path = str(cfg.get("output_folder", "auto")).strip()

    # åˆ¤æ–­æ˜¯å¦éœ€è¦è‡ªåŠ¨æ¨å¯¼
    need_infer_img = raw_img_path.lower() in ["auto", "none", "null"]
    need_infer_out = raw_out_path.lower() in ["auto", "none", "null"]

    # å¦‚æœä¸¤ä¸ªéƒ½æŒ‡å®šäº†å…·ä½“è·¯å¾„ï¼Œç›´æ¥è¿”å›ï¼Œä¸æµªè´¹æ—¶é—´è§£æ
    if not need_infer_img and not need_infer_out:
        return cfg

    # --- å¼€å§‹è§£æ ckpt_path ---
    ckpt_path = Path(cfg.ckpt_path)
    parts = ckpt_path.parts

    try:
        # 1. å®šä½ "checkpoints" æ–‡ä»¶å¤¹çš„ä½ç½®
        ckpt_idx = parts.index("checkpoints")
    except ValueError:
        logger.warning("âš ï¸ æ— æ³•è‡ªåŠ¨æ¨å¯¼è·¯å¾„ï¼šæƒé‡è·¯å¾„ä¸­æœªåŒ…å« 'checkpoints' æ–‡ä»¶å¤¹ã€‚å°†ç»´æŒåŸå§‹é…ç½®ã€‚")
        return cfg

    # 2. æ¨å¯¼ Project Root (local_results çš„ä¸Šä¸€çº§)
    # parts[:ckpt_idx] æ˜¯ .../local_results/
    # parts[:ckpt_idx-1] æ˜¯ .../Project/Base-model/
    project_root = Path(*parts[:ckpt_idx - 1])

    # 3. æå–æ•°æ®é›†åç§° (checkpoints å’Œ run_folder ä¹‹é—´çš„éƒ¨åˆ†)
    # ç»“æ„: .../checkpoints/{æ•°æ®é›†}/{å­æ•°æ®é›†}/{è¿è¡Œæ–‡ä»¶å¤¹}/{æƒé‡æ–‡ä»¶}
    # parts[-2] æ˜¯è¿è¡Œæ–‡ä»¶å¤¹ (ä¾‹å¦‚ base_loss_3shot_...)
    run_folder_name = parts[-2]
    dataset_rel_parts = parts[ckpt_idx + 1: -2]
    dataset_rel_path = Path(*dataset_rel_parts)

    # 4. æå– Shot æ•° (ç”¨äºè¾“å‡ºæ–‡ä»¶å¤¹å‘½å)
    match = re.search(r'(\d+)shot', run_folder_name)
    shot_num = match.group(1) if match else "0"

    # --- æ‰§è¡Œèµ‹å€¼ ---

    # (A) è‡ªåŠ¨æ¨å¯¼ image_path
    if need_infer_img:
        # è§„åˆ™: é¡¹ç›®æ ¹ç›®å½•/datasets/{æ•°æ®é›†è·¯å¾„}/test
        autogen_image_path = project_root / "datasets" / dataset_rel_path / "test"
        cfg.image_path = str(autogen_image_path)
        logger.info(f"âš¡ [Auto] Image Path æ¨å¯¼ä¸º: {cfg.image_path}")
    else:
        logger.info(f"ğŸ“ [Manual] Image Path ä½¿ç”¨æŒ‡å®šè·¯å¾„: {cfg.image_path}")

    # (B) è‡ªåŠ¨æ¨å¯¼ output_folder
    if need_infer_out:
        # è§„åˆ™: é¡¹ç›®æ ¹ç›®å½•/local_results/output/{æ•°æ®é›†è·¯å¾„}/{shot}_shot_test
        autogen_output_folder = project_root / "local_results" / "output" / dataset_rel_path / f"{shot_num}_shot_test"
        cfg.output_folder = str(autogen_output_folder)
        logger.info(f"âš¡ [Auto] Output Folder æ¨å¯¼ä¸º: {cfg.output_folder}")
    else:
        logger.info(f"ğŸ“ [Manual] Output Folder ä½¿ç”¨æŒ‡å®šè·¯å¾„: {cfg.output_folder}")

    cfg.shot_name= shot_num

    return cfg



@hydra.main(config_path="../configs", config_name="tem_infer", version_base="1.3.2")
def main(cfg):

    # ã€ç¬¬ä¸€æ­¥ã€‘è°ƒç”¨è‡ªåŠ¨æ¨å¯¼å‡½æ•°
    cfg = auto_infer_paths(cfg)

    # 1. è·å–æ‰€æœ‰æ•°æ®è·¯å¾„
    all_image_paths, all_mask_paths = get_paths_nested(cfg)
    total_samples = len(all_image_paths)

    # æå–æ•°æ®é›†åç§°ï¼ˆç”¨äºCSVå‘½åï¼‰
    try:
        dataset_name = Path(cfg.image_path).parent.name
    except:
        dataset_name = "Dataset"

    logger.info("=" * 80)
    logger.info(f"ğŸš€ å¼€å§‹æ¨ç†ä»»åŠ¡ (Dataset: {dataset_name})")
    logger.info(f"ğŸ“‚ æƒé‡è·¯å¾„: {cfg.ckpt_path}")
    logger.info(f"ğŸ“‚ ä¿å­˜CSVè‡³: ./local_results/tem_infer/{dataset_name}")
    logger.info("=" * 80)

    # 2. è·å–å¯ç”¨ GPU åˆ—è¡¨
    if not cfg.get("gpus"):
        logger.warning("æœªé…ç½® gpus åˆ—è¡¨ï¼Œå°†å°è¯•ä½¿ç”¨ cuda:0")
        target_gpus = [0]
    else:
        target_gpus = list(cfg.gpus)

    num_gpus = len(target_gpus)

    # 3. æ•°æ®åˆ†ç‰‡
    chunk_size = math.ceil(total_samples / num_gpus)
    chunks_img = [all_image_paths[i:i + chunk_size] for i in range(0, total_samples, chunk_size)]

    if all_mask_paths:
        chunks_mask = [all_mask_paths[i:i + chunk_size] for i in range(0, total_samples, chunk_size)]
    else:
        chunks_mask = [None] * len(chunks_img)

    # 4. å¯åŠ¨å¤šè¿›ç¨‹
    manager = mp.Manager()
    return_dict = manager.dict()
    processes = []

    for rank, gpu_id in enumerate(target_gpus):
        if rank >= len(chunks_img):
            break

        p = mp.Process(
            target=run_inference_worker,
            args=(
                rank,
                gpu_id,
                chunks_img[rank],
                chunks_mask[rank],
                cfg,
                return_dict
            )
        )
        p.start()
        processes.append(p)

    for p in processes:
        p.join()

    # 5. æ±‡æ€»ç»“æœä¸è®°å½•
    logger.info("æ‰€æœ‰è¿›ç¨‹å·²å®Œæˆï¼Œæ­£åœ¨æ±‡æ€»æŒ‡æ ‡...")

    final_metrics_dict = {}
    for rank, metrics in return_dict.items():
        final_metrics_dict.update(metrics)

    if final_metrics_dict:
        # è®¡ç®—æ€»ä½“å¹³å‡
        mean_metrics = calculate_mean_metrics(list(final_metrics_dict.values()), round_to=cfg.round_to)

        # æ‰“å°æ§åˆ¶å°ç®€æŠ¥
        logger.info("=" * 60)
        logger.info(f"ğŸ FINAL GLOBAL AVERAGE METRICS ({len(final_metrics_dict)} cases):")
        logger.info("=" * 60)
        # æ‰“å°æ‰€æœ‰å¹³å‡æŒ‡æ ‡
        for key in sorted(mean_metrics.keys()):
            val = mean_metrics[key]
            val = val.item() if hasattr(val, 'item') else val
            logger.info(f"Mean {key:<25}: {val:.4f}")
        logger.info("=" * 60)

        # ä¿å­˜å¢å¼ºç‰ˆ CSV æŠ¥å‘Š (è‡ªåŠ¨åŒ…å« cldice)
        save_csv_report(final_metrics_dict, mean_metrics, cfg, dataset_name)

    else:
        logger.info("æ²¡æœ‰äº§ç”Ÿè¯„ä¼°æŒ‡æ ‡ (å¯èƒ½æœªæä¾›æ ‡ç­¾æˆ– mask_paths ä¸ºç©º)")

    logger.info("Global Inference finished.")

if __name__ == "__main__":
    main()